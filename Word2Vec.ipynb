{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80387ad",
   "metadata": {},
   "source": [
    "### Iimport libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0113bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import yaml\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Dropout\n",
    "from keras.preprocessing import text\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import losses, optimizers\n",
    "from tensorflow.keras.activations import softmax\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fdfc9",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c09c6622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Head------------------\n",
      "   ARTICLE_ID      TITLE                 SECTION_TITLE  \\\n",
      "0           0  Anarchism                  Introduction   \n",
      "1           0  Anarchism     Etymology and terminology   \n",
      "2           0  Anarchism                       History   \n",
      "3           0  Anarchism  Anarchist schools of thought   \n",
      "4           0  Anarchism   Internal issues and debates   \n",
      "\n",
      "                                        SECTION_TEXT  \n",
      "0  \\n\\n\\n\\n\\n\\n'''Anarchism''' is a political phi...  \n",
      "1  \\n\\nThe term ''anarchism'' is a compound word ...  \n",
      "2  \\n\\n===Origins===\\nWoodcut from a Diggers docu...  \n",
      "3  \\nPortrait of philosopher Pierre-Joseph Proudh...  \n",
      "4  \\nconsistent with anarchist values is a contro...  \n",
      "------------------Tail------------------\n",
      "        ARTICLE_ID              TITLE    SECTION_TITLE  \\\n",
      "265134       30475  Triboluminescence  Further reading   \n",
      "265135       30475  Triboluminescence   External links   \n",
      "265136       30476       Markov chain     Introduction   \n",
      "265137       30476       Markov chain     Introduction   \n",
      "265138       30476       Markov chain          History   \n",
      "\n",
      "                                             SECTION_TEXT  \n",
      "265134                                           * \\n* \\n  \n",
      "265135  \\n* \\n* \\n*  Triboluminescence Discussion on T...  \n",
      "265136  \\nIn probability theory and related fields, a ...  \n",
      "265137  Russian mathematician Andrey Markov.\\nA Markov...  \n",
      "265138  Andrey Markov studied Markov chains in the ear...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('enwiki.csv')\n",
    "print('------------------Head------------------')\n",
    "print(df.head())\n",
    "print('------------------Tail------------------')\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f55548",
   "metadata": {},
   "source": [
    "### Remove unnecessary data from dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795dbd54",
   "metadata": {},
   "source": [
    "# Set constrains for filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db94d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc946361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 0 for taking all data\n",
    "data_limit = 300 \n",
    "max_sentence_lenght = 1000\n",
    "max_word_count = 100\n",
    "min_word_count = 5\n",
    "max_sentence = 1000\n",
    "vocabulary_size = 30\n",
    "window_size = 4\n",
    "\n",
    "\n",
    "data_info['data_limit']= data_limit\n",
    "data_info['max_sentence_lenght'] = max_sentence_lenght\n",
    "data_info['max_word_count'] = max_word_count \n",
    "data_info['min_word_count'] = min_word_count\n",
    "data_info['max_sentence'] = max_sentence \n",
    "data_info['vocabulary_size'] = vocabulary_size\n",
    "data_info['window_size'] = window_size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78849ce5",
   "metadata": {},
   "source": [
    "### Create a dictioanry for saving the data related information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a027071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total section texts -----> 265139\n"
     ]
    }
   ],
   "source": [
    "df.drop(['ARTICLE_ID', 'TITLE', 'SECTION_TITLE'], axis=1)\n",
    "section_texts = df['SECTION_TEXT'].apply(str)\n",
    "print('Total section texts ----->', len(section_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6f4f6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total section texts -----> 300\n"
     ]
    }
   ],
   "source": [
    "if data_limit != 0:\n",
    "    section_texts = section_texts[:data_limit]\n",
    "print('Total section texts ----->', len(section_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7340200",
   "metadata": {},
   "source": [
    "### Convert text to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c779773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for texts in section_texts:\n",
    "    for sentence in sent_tokenize(texts):\n",
    "        if len(sentence) < max_sentence_lenght:\n",
    "            sentences.append(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a62ec2df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences 6661\n"
     ]
    }
   ],
   "source": [
    "total_sentences = len(sentences)\n",
    "print('Total sentences', total_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d56d9",
   "metadata": {},
   "source": [
    "### Convert sentences to words and create vocabulary with frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c794f606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "word_list = []\n",
    "vocabulary_with_frequency = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    words_without_stop_words = [word for word in words if word.isalpha() and word not in stop_words and len(word) != 1]\n",
    "    \n",
    "    word_lenght = len(words_without_stop_words)\n",
    "    if word_lenght <= max_word_count and word_lenght >= min_word_count:\n",
    "        word_list.append(words_without_stop_words)\n",
    "        \n",
    "        for word in words_without_stop_words:\n",
    "            if word not in vocabulary_with_frequency.keys():\n",
    "                vocabulary_with_frequency[word] = 1\n",
    "            else:\n",
    "                vocabulary_with_frequency[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015c0bd",
   "metadata": {},
   "source": [
    "### Total words and vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c459fec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in corpus 80055\n",
      "Vocabulary size 15960\n"
     ]
    }
   ],
   "source": [
    "# After filtering the words\n",
    "total_sentences = len(word_list)\n",
    "\n",
    "total_words = 0\n",
    "for words in word_list:\n",
    "    total_words += len(words)\n",
    "print('Total words in corpus', total_words)\n",
    "print('Vocabulary size', len(vocabulary_with_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebb1bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocabulary_with_frequency = sorted(vocabulary_with_frequency.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b6cc6",
   "metadata": {},
   "source": [
    "### Remove less freuquent word form dictionary and assign unique id to vocabulary word\n",
    "\n",
    "\n",
    "<br> \n",
    "Create two vocabulary.<br> \n",
    "<b>word_to_id</b> for gettig the word for an id. <br>\n",
    "<b>id_to_word</b> for gettign the id for word. <br>\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2588df39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'PAD'), ('lincoln', 1), ('also', 2), ('one', 3), ('apollo', 4), ('state', 5), ('first', 6), ('achilles', 7), ('alabama', 8), ('aristotle', 9), ('time', 10), ('autism', 11), ('many', 12), ('new', 13), ('century', 14)]\n",
      "-------------------------------------\n",
      "[('PAD', 0), (1, 'lincoln'), (2, 'also'), (3, 'one'), (4, 'apollo'), (5, 'state'), (6, 'first'), (7, 'achilles'), (8, 'alabama'), (9, 'aristotle'), (10, 'time'), (11, 'autism'), (12, 'many'), (13, 'new'), (14, 'century')]\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {}\n",
    "word_to_id[0] = 'PAD'\n",
    "id_to_word = {}\n",
    "id_to_word['PAD'] = 0\n",
    "word_id = 1\n",
    "\n",
    "for word, _ in sorted_vocabulary_with_frequency:\n",
    "    if word_id < vocabulary_size:\n",
    "        word_to_id[word] = word_id\n",
    "        id_to_word[word_id] = word\n",
    "        word_id += 1\n",
    "        \n",
    "print(list(islice(word_to_id.items(), 15)))\n",
    "print('-------------------------------------')\n",
    "print(list(islice(id_to_word.items(), 15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92629475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dictionary size 30\n"
     ]
    }
   ],
   "source": [
    "print('New dictionary size', vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3b870",
   "metadata": {},
   "source": [
    "### Convert word_list to word_id_list for expressign the words of a sentece using vocabulary id\n",
    "\n",
    "<br> Remove the words which are not present in dictionary</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcb0dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info['total sentence'] = total_sentences\n",
    "data_info['total words'] = total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c02693f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering total sentenc 6087\n",
      "Before filtering total words 80055\n",
      "--------------------------------------------\n",
      "After filtering total sentence 82\n",
      "After filtering total words 486\n"
     ]
    }
   ],
   "source": [
    "print('Before filtering total sentenc', total_sentences)\n",
    "print('Before filtering total words', total_words)\n",
    "\n",
    "sentece_word_ids = []\n",
    "total_sentences = 0\n",
    "total_words = 0\n",
    "\n",
    "for words in word_list:\n",
    "    filtered_words_ids = [word_to_id[word] for word in words if word in word_to_id.keys()]\n",
    "    words_in_current_sentece = len(filtered_words_ids)\n",
    "    \n",
    "    if words_in_current_sentece >= min_word_count:\n",
    "        total_sentences += 1\n",
    "        total_words += words_in_current_sentece\n",
    "        sentece_word_ids.append(filtered_words_ids)\n",
    "        \n",
    "print('--------------------------------------------')\n",
    "print('After filtering total sentence', total_sentences)\n",
    "print('After filtering total words', total_words)\n",
    "\n",
    "data_info['after_filtering_total_sentencs'] = total_sentences\n",
    "data_info['after_filtering_total_words'] = total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211b3a88",
   "metadata": {},
   "source": [
    "### Untill Now \n",
    "<br> \n",
    "<b>sentece_word_ids:</b> Sentence wise words's id. <br>\n",
    "<b>word_to_id:</b> Vocabulary for getting the <b>word_id</b> for a <b>word</b>.<br>\n",
    "<b>id_to_word:</b> Vocabulary for getting the <b>word</b> for a <b> word_id</b>.<br>\n",
    "<b>positive_skip_grams:</b> skipgrams of target and context word pairs.\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da05f647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total senteces 82\n",
      "Total words 486\n",
      "Total unique words in vocabulary 30\n"
     ]
    }
   ],
   "source": [
    "print('Total senteces', total_sentences)\n",
    "print('Total words', total_words)\n",
    "vocabulary_size = len(word_to_id)\n",
    "print('Total unique words in vocabulary', vocabulary_size)\n",
    "\n",
    "data_info['total_unique words in vocabulary'] = vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bedcf3",
   "metadata": {},
   "source": [
    "### Save data for later use\n",
    "<br> It will help use to skip the data processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78be2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_data(data, to_file):\n",
    "#     if os.path.exists(to_file+'.yaml'):\n",
    "#         os.remove(to_file+'.yaml')\n",
    "    \n",
    "#     with open(to_file+'.yaml', 'w') as file:\n",
    "#         documents = yaml.dump(data, file, sort_keys=False)\n",
    "    \n",
    "# def load_data(from_file):\n",
    "#     with open(from_file+'.yaml') as file:\n",
    "#         return yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7c7fc",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0f86fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_data(word_to_id, 'word_to_id')\n",
    "# save_data(id_to_word, 'id_to_word')\n",
    "# save_data(sentece_word_ids, 'sentece_word_ids')\n",
    "# # save_data(positive_skip_grams, 'positive_skip_grams')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca271ed1",
   "metadata": {},
   "source": [
    "# ⚠️⚠️⚠️ Reset Everything ⚠️⚠️⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dd4a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8e0a5",
   "metadata": {},
   "source": [
    "# Start again 🏃‍♂️🏃‍♂️🏃‍♂️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ee5f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import sys\n",
    "# import yaml\n",
    "# import os\n",
    "# import io\n",
    "\n",
    "# import nltk\n",
    "# from nltk.corpus import PlaintextCorpusReader\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# from tensorflow.keras import layers\n",
    "# from keras.preprocessing import text\n",
    "\n",
    "# from itertools import islice\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from tensorflow.keras.layers import Embedding, Input, Dense, Dropout\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras import losses, optimizers\n",
    "# from tensorflow.keras.activations import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbfdc047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_data(data, to_file):\n",
    "#     if os.path.exists(to_file+'.yaml'):\n",
    "#         os.remove('id_to_word.yaml')\n",
    "    \n",
    "#     with open(to_file+'.yaml', 'w') as file:\n",
    "#         documents = yaml.dump(data, file, sort_keys=False)\n",
    "    \n",
    "# def load_data(from_file):\n",
    "#     with open(from_file+'.yaml') as file:\n",
    "#         return yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b374e",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1927780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_to_word = load_data('id_to_word')\n",
    "# word_to_id = load_data('word_to_id')\n",
    "# sentece_word_ids = load_data('sentece_word_ids')\n",
    "# # positive_skip_grams = load_data('positive_skip_grams')\n",
    "# vocabulary_size = len(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1005f",
   "metadata": {},
   "source": [
    "#### Example after loadign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec6d81a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PAD', 0), (1, 'lincoln'), (2, 'also'), (3, 'one'), (4, 'apollo'), (5, 'state'), (6, 'first'), (7, 'achilles'), (8, 'alabama'), (9, 'aristotle')]\n",
      "------------------------------------------\n",
      "[(0, 'PAD'), ('lincoln', 1), ('also', 2), ('one', 3), ('apollo', 4), ('state', 5), ('first', 6), ('achilles', 7), ('alabama', 8), ('aristotle', 9)]\n",
      "------------------------------------------\n",
      "[[16, 12, 26, 14, 24, 24], [5, 24, 19, 5, 6, 19], [24, 12, 24, 25, 24], [3, 24, 6, 24, 10], [6, 29, 24, 6, 24]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(list(islice(id_to_word.items(), 10)))\n",
    "print('------------------------------------------')\n",
    "print(list(islice(word_to_id.items(), 10)))\n",
    "print('------------------------------------------')\n",
    "print(sentece_word_ids[:5])\n",
    "print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adc2d5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929921f9",
   "metadata": {},
   "source": [
    "### Generate 2D `vocabulary_size` list for getting the one-hot vector for a index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d093fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = [[0 for i in range(vocabulary_size)] for j in range(vocabulary_size)]\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    one_hot[index][index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a985fc",
   "metadata": {},
   "source": [
    "### Helper Function\n",
    "\n",
    "In every cases name will be `pari`, `skip` or `cbow`.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abf604e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_name(name):\n",
    "    if name == 'pair':\n",
    "        return 'save/model_pair/'\n",
    "    elif name == 'skip':\n",
    "        'save/model_skip/'\n",
    "    elif name == 'cbow':\n",
    "        return 'save/model_cbow/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4f252cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_file_name():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S-%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d74957e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(name, model, file_name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    model.save(path+file_name+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9ef6f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name, file_name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if name == 'cbow':\n",
    "        return keras.models.load_model(path+file_name+'.h5', custom_objects={\"Average\": Average})\n",
    "    \n",
    "    return tf.keras.models.load_model(path+file_name+'.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0193aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_list(name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        return os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c21e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_info(name, file_name, data):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "            \n",
    "    with open(path+file_name+'.yaml', 'w') as file:\n",
    "        documents = yaml.dump(data, file, sort_keys=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a79ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_list(name):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        return os.listdir(path)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "295286d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(name, file_name):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "            with open(path+file_name+'.yaml') as file:\n",
    "                return yaml.load(file, Loader=yaml.Loader)\n",
    "            \n",
    "#             yaml.load('Foo: !Ref bar', Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08e575df",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = get_unique_file_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de79e51",
   "metadata": {},
   "source": [
    "# `Model - Pair Input`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbff3b2",
   "metadata": {},
   "source": [
    "### Save data_info to data_info_pair.\n",
    "<br> Because `data_info` is common for all model</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a2c1988",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_pair = data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e3e0dd",
   "metadata": {},
   "source": [
    "### Create (taget, context) pair for all of the words and convert them into one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9aea7804",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2248, 30)\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for ids in sentece_word_ids:\n",
    "    skip, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "        ids,\n",
    "        vocabulary_size,\n",
    "        window_size=window_size, negative_samples=0,\n",
    "        shuffle=False,\n",
    "        categorical=True)\n",
    "    for target, context in skip:\n",
    "        X.append(one_hot[target])\n",
    "        Y.append(one_hot[context])\n",
    "print(np.array(X).shape)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43c2d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_pair['Total target and context word pairs'] = len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a2d480",
   "metadata": {},
   "source": [
    "### Split training testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c54b3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858d9a7d",
   "metadata": {},
   "source": [
    "### Pairwise target, context model\n",
    "<br>\n",
    "Input target word in one-hot encoding format. <br>\n",
    "Output context word of corespoding target word in one-hot encoding format. <br>\n",
    "\n",
    "All of the targets and their corresponding context words are fitting at once. \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d52257e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding_pair (Dense)   (None, 200)               6200      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 12,230\n",
      "Trainable params: 12,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 22:25:06.991662: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-11-18 22:25:06.992505: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(vocabulary_size, ))\n",
    "x = Dense(200, name='w2v_embedding_pair')(inp)\n",
    "x = Dense(vocabulary_size, activation='softmax')(x)\n",
    "\n",
    "model_pair = Model(inputs=inp, outputs=x)\n",
    "model_pair.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2befd208",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_started = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b298f364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 06:05:07.706208: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-11-18 06:05:07.706411: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2021-11-18 06:05:07.896299: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 2.8277 - accuracy: 0.2729\n",
      "Epoch 2/3\n",
      "1376/1376 [==============================] - 6s 5ms/step - loss: 2.6106 - accuracy: 0.3132\n",
      "Epoch 3/3\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 2.5720 - accuracy: 0.3096\n"
     ]
    }
   ],
   "source": [
    "model_pair.compile(loss=losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "history_pair = model_pair.fit(X_train, Y_train, epochs = 3, batch_size=2, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee9db39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end = datetime.datetime.now()\n",
    "data_info_pair['training time'] =  training_end - training_started\n",
    "data_info_pair['pair model training history'] = history_pair.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7603684",
   "metadata": {},
   "source": [
    "### Save Pair Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd98a951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "save_model('pair', model_pair, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f221c91e",
   "metadata": {},
   "source": [
    "### Testing Pair Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "415534c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 - 0s - loss: 2.7699 - accuracy: 0.2703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 06:05:26.470545: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_pair.evaluate(X_test, Y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b57bfc34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated loss 2.7699196338653564 and accuracy 0.27034884691238403\n"
     ]
    }
   ],
   "source": [
    "data_info_pair['pair model evaluatation loss'] = loss\n",
    "data_info_pair['pair model evaluatation accuracy'] = accuracy\n",
    "print('Evaluated loss',loss, 'and accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc5387",
   "metadata": {},
   "source": [
    "### Save Pair Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4dd49408",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info('pair',file_name, data_info_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61167fd7",
   "metadata": {},
   "source": [
    "### Store the vector and metadata for Pair Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00715211",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model_pair.get_layer('w2v_embedding_pair').get_weights()[0]\n",
    "\n",
    "path = get_path_name('pair') + 'vector_metadata/'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "out_v = io.open(path+file_name+'vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open(path+file_name+'metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(id_to_word[index] + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0636d03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Pair Model\n"
     ]
    }
   ],
   "source": [
    "print('End Pair Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4465d45",
   "metadata": {},
   "source": [
    "# Data Processing for the CBOW and SKIP-GRAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfe7554",
   "metadata": {},
   "source": [
    "<br> This function creates the target words and all of it's corrosponding context words.<br>\n",
    "return:<br>\n",
    "------<b>target_id:</b> id of the target word without encoding.<br>\n",
    "------<b>context_ids:</b> All of the context words for a target_id<br>\n",
    "\n",
    "<b>Note:</b> Zero padding is being used for returing total `window_size * 2` context words for a target word.<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d003cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_target_contexts(word_ids, window_size):\n",
    "    target_id = []\n",
    "    context_ids = []\n",
    "\n",
    "    one_hot_target_id = []\n",
    "    one_hot_context_ids = []\n",
    "\n",
    "    for ids in word_ids:\n",
    "        for index, word_id in enumerate(ids):\n",
    "            if not word_id:\n",
    "                continue\n",
    "                \n",
    "            window_start = max(0, index - window_size)\n",
    "            window_end = min(len(ids), index + window_size + 1)\n",
    "            \n",
    "            target_id.append(word_id)\n",
    "            context_ids.append([ids[window_index] for window_index in range(window_start, window_end) if window_index != index])\n",
    "                    \n",
    "    return target_id, pad_sequences(context_ids, maxlen = window_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8e8931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01d545b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id, context_ids =  get_target_contexts(sentece_word_ids, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b24276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info['Target words'] = len(target_id)\n",
    "data_info['Context words with zero padding'] = len(context_ids) * window_size * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f3453c",
   "metadata": {},
   "source": [
    "### Express `target_id` &  `context_ids` using one-hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26160bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_target_id = [one_hot[id] for id in target_id]\n",
    "one_hot_context_ids = []\n",
    "\n",
    "for context_id in context_ids:\n",
    "    one_hot_context_ids.append([one_hot[id] for id in context_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7bdb3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for context in one_hot_context_ids:\n",
    "    row = [0] * vocabulary_size\n",
    "    for word in context:\n",
    "        for idx in range(vocabulary_size):\n",
    "            row[idx] += word[idx]\n",
    "    X.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff14e2",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0507f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_cbow =  data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a09cc4",
   "metadata": {},
   "source": [
    "### Split training testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad66e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, one_hot_target_id, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3318de",
   "metadata": {},
   "source": [
    "#### Extending keras layer for taking the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d96af11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Average(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Average, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.divide(inputs, window_size*2)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a15ab51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding (Dense)        (None, 200)               6200      \n",
      "_________________________________________________________________\n",
      "average_1 (Average)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 12,230\n",
      "Trainable params: 12,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(vocabulary_size, ))\n",
    "x = Dense(200, name='w2v_embedding')(inp)\n",
    "x = Average(200)(x)\n",
    "x = Dense(vocabulary_size, activation='softmax')(x)\n",
    "\n",
    "model_cbow = Model(inputs=inp, outputs=x)\n",
    "model_cbow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1b5f942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_started = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a971e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "28/36 [======================>.......] - ETA: 0s - loss: 3.3868 - accuracy: 0.1643"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 23:26:01.407404: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 6ms/step - loss: 3.3816 - accuracy: 0.1813\n",
      "Epoch 2/3\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 3.3330 - accuracy: 0.2875\n",
      "Epoch 3/3\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 3.2719 - accuracy: 0.2875\n"
     ]
    }
   ],
   "source": [
    "model_cbow.compile(loss=losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "history_cbow = model_cbow.fit(X_train, Y_train, epochs = 3, batch_size=50, verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0ac711ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end = datetime.datetime.now()\n",
    "data_info_cbow['training time'] =  training_end - training_started\n",
    "data_info_cbow['cbow model training history'] = history_cbow.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5237b",
   "metadata": {},
   "source": [
    "### Save CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b33e8ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "save_model('cbow', model_pair, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439e276",
   "metadata": {},
   "source": [
    "### Testing CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b76ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_cbow.evaluate(X_test, Y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f02dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_cbow['cbow model evaluatation loss'] = loss\n",
    "data_info_cbow['cbow model evaluatation accuracy'] = accuracy\n",
    "print('Evaluated loss',loss, 'and accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a27f50d",
   "metadata": {},
   "source": [
    "### Save CBOW model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "97083c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info('cbow',file_name, data_info_cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db2008",
   "metadata": {},
   "source": [
    "## SKIP-GRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec85e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flatten = [sum(one_hot_context_id, []) for one_hot_context_id in one_hot_context_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a285d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(vocabulary_size, ))\n",
    "x = Dense(200, name='w2v_embedding')(inp)\n",
    "x = Dense(vocabulary_size * window_size * 2, activation='softmax')(x)\n",
    "model_skip = Model(inputs=inp, outputs=x)\n",
    "model_skip.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f199d529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_skip.compile(loss=losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "model_skip.fit(one_hot_target_id, X_flatten, epochs = 5, batch_size=50, verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc896b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_model(model_name, model, file_name):\n",
    "#     if model_name == 'pair'\n",
    "#     model_name.save(path+'/'+file_name+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b800409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_info_list(path):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a67a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skip.save('model_skip/model_skip/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa28cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skip.load('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248754f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('model_skip/model_skip/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27334d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb01bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = new_model.evaluate(one_hot_target_id, X_flatten, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb64d7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflowEnv] *",
   "language": "python",
   "name": "conda-env-tensorflowEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

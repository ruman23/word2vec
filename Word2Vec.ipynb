{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80387ad",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0113bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import losses, optimizers\n",
    "from tensorflow.keras.activations import softmax\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d927f",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c09c6622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Head------------------\n",
      "   Unnamed: 0     id                                              title  \\\n",
      "0           0  17283  House Republicans Fret About Winning Their Hea...   \n",
      "1           1  17284  Rift Between Officers and Residents as Killing...   \n",
      "2           2  17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
      "3           3  17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
      "4           4  17287  Kim Jong-un Says North Korea Is Preparing to T...   \n",
      "\n",
      "      publication                         author        date    year  month  \\\n",
      "0  New York Times                     Carl Hulse  2016-12-31  2016.0   12.0   \n",
      "1  New York Times  Benjamin Mueller and Al Baker  2017-06-19  2017.0    6.0   \n",
      "2  New York Times                   Margalit Fox  2017-01-06  2017.0    1.0   \n",
      "3  New York Times               William McDonald  2017-04-10  2017.0    4.0   \n",
      "4  New York Times                  Choe Sang-Hun  2017-01-02  2017.0    1.0   \n",
      "\n",
      "   url                                            content  \n",
      "0  NaN  WASHINGTON  —   Congressional Republicans have...  \n",
      "1  NaN  After the bullet shells get counted, the blood...  \n",
      "2  NaN  When Walt Disney’s “Bambi” opened in 1942, cri...  \n",
      "3  NaN  Death may be the great equalizer, but it isn’t...  \n",
      "4  NaN  SEOUL, South Korea  —   North Korea’s leader, ...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('articles1.csv')\n",
    "print('------------------Head------------------')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795dbd54",
   "metadata": {},
   "source": [
    "# Setting Constraints for Filtering the Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc946361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `data_limit` zero (0) for processing all of the data.\n",
    "data_limit = 10000\n",
    "# If the fruquency count of a word is less than the `min_frquency_of_word` than it will be removed from the dictionary\n",
    "min_frquency_of_word = 300 \n",
    "window_size = 7\n",
    "max_sentence_lenght = 1000\n",
    "max_word_count = 100\n",
    "min_word_count = 5\n",
    "max_sentence = 1000\n",
    "\n",
    "data_info = {}\n",
    "data_info['data_limit']= data_limit\n",
    "data_info['min_frquency_of_word'] = min_frquency_of_word\n",
    "data_info['window_size'] = window_size \n",
    "data_info['max_sentence_lenght'] = max_sentence_lenght\n",
    "data_info['max_word_count'] = max_word_count \n",
    "data_info['min_word_count'] = min_word_count\n",
    "data_info['max_sentence'] = max_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a027071",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_texts = df['content'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffda0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f4f6a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total texts-----> 1000\n"
     ]
    }
   ],
   "source": [
    "if data_limit != 0:\n",
    "    section_texts = section_texts[:data_limit]\n",
    "print('Total texts----->', len(section_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01f28371",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace7cd0",
   "metadata": {},
   "source": [
    "### Downloading `nltk` Resources for Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67bc505d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ruman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ruman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7340200",
   "metadata": {},
   "source": [
    "### Converting Texts to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c779773b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for texts in section_texts:\n",
    "    for sentence in sent_tokenize(texts):\n",
    "        if len(sentence) < max_sentence_lenght:\n",
    "            sentences.append(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a62ec2df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences 53139\n"
     ]
    }
   ],
   "source": [
    "total_sentences = len(sentences)\n",
    "print('Total sentences', total_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64a56a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "del texts\n",
    "del section_texts\n",
    "del max_sentence_lenght\n",
    "del sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d56d9",
   "metadata": {},
   "source": [
    "### Filtering out the Sentences, Converting Sentences to Words and Creating Vocabulary with Frequency\n",
    "\n",
    "For filtering out-\n",
    "<ul>\n",
    "  <li>Remove the <b>stop words</b>.</li>\n",
    "  <li>Remove the word if it is not an <b>alpha</b>.</li>\n",
    "  <li>Remove the word if number of character is less than <b>one</b>.</li>\n",
    "  <li>Remove sentences if word count is greater than <b>max_word_count</b> or less than <b>min_word_count</b></li>    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c794f606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "word_list = []\n",
    "vocabulary_with_frequency = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    words_without_stop_words = [word for word in words if word.isalpha() and word not in stop_words and len(word) != 1]\n",
    "    \n",
    "    word_lenght = len(words_without_stop_words)\n",
    "    if word_lenght <= max_word_count and word_lenght >= min_word_count:\n",
    "        word_list.append(words_without_stop_words)\n",
    "        \n",
    "        for word in words_without_stop_words:\n",
    "            if word not in vocabulary_with_frequency.keys():\n",
    "                vocabulary_with_frequency[word] = 1\n",
    "            else:\n",
    "                vocabulary_with_frequency[word] += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be6b93cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36978\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary_with_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4477cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del stop_words\n",
    "del stopwords\n",
    "del sentences\n",
    "del sentence\n",
    "del words\n",
    "del words_without_stop_words\n",
    "del word_lenght\n",
    "del word\n",
    "del max_sentence\n",
    "del max_word_count\n",
    "del nltk\n",
    "del word_tokenize\n",
    "del total_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015c0bd",
   "metadata": {},
   "source": [
    "#### Total Words and Vocabularies After Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c459fec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in corpus 572810\n",
      "Vocabulary size 36978\n"
     ]
    }
   ],
   "source": [
    "total_sentences = len(word_list)\n",
    "\n",
    "total_words = 0\n",
    "for words in word_list:\n",
    "    total_words += len(words)\n",
    "print('Total words in corpus', total_words)\n",
    "print('Vocabulary size', len(vocabulary_with_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af2cc722",
   "metadata": {},
   "outputs": [],
   "source": [
    "del words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf64ad",
   "metadata": {},
   "source": [
    "### Sorting Vocabulary Based on Frequency Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebb1bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocabulary_with_frequency = sorted(vocabulary_with_frequency.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df438285",
   "metadata": {},
   "source": [
    "### Removing the Word Form the Dictioary if Frequency count is Less Than `min_frquency_of_word`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6da80de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocabulary = {key: frequency for key, frequency in sorted_vocabulary_with_frequency if frequency > min_frquency_of_word}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f61f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sorted_vocabulary_with_frequency\n",
    "del vocabulary_with_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b6cc6",
   "metadata": {},
   "source": [
    "### Removing the Less Freuquent Word form Dictionary and Assigning Unique Id to Each Word\n",
    "\n",
    "Also created two dictionaries-\n",
    "<ul>\n",
    "  <li><b>word_to_id:</b> to get word for a word id.</li>\n",
    "  <li><b>id_to_word:</b> to get the id for word.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2588df39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'PAD'), ('said', 1), ('trump', 2), ('would', 3), ('one', 4), ('new', 5), ('people', 6), ('president', 7), ('like', 8), ('also', 9), ('states', 10), ('united', 11), ('years', 12), ('many', 13), ('could', 14)]\n",
      "-------------------------------------\n",
      "[('PAD', 0), (1, 'said'), (2, 'trump'), (3, 'would'), (4, 'one'), (5, 'new'), (6, 'people'), (7, 'president'), (8, 'like'), (9, 'also'), (10, 'states'), (11, 'united'), (12, 'years'), (13, 'many'), (14, 'could')]\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {}\n",
    "word_to_id[0] = 'PAD'\n",
    "id_to_word = {}\n",
    "id_to_word['PAD'] = 0\n",
    "word_id = 1\n",
    "\n",
    "for word in sorted_vocabulary:\n",
    "    word_to_id[word] = word_id\n",
    "    id_to_word[word_id] = word\n",
    "    word_id += 1\n",
    "print(list(islice(word_to_id.items(), 15)))\n",
    "print('-------------------------------------')\n",
    "print(list(islice(id_to_word.items(), 15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c6584aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sorted_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3b870",
   "metadata": {},
   "source": [
    "### Converting `word_list` to `word_id_list` for Expressign the Words by Unique Dincatioanry ID  \n",
    "In this step I have also--\n",
    "<ul>\n",
    "  <li>Removed a word if that is not in the dictionary</li>\n",
    "  <li>Removed a sentece if word count is less than the <b>min_word_count</b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcb0dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences before filtering 43998\n",
      "Total words before filtering 572810\n"
     ]
    }
   ],
   "source": [
    "data_info['Total sentences before filtering'] = total_sentences\n",
    "data_info['Total words before filtering'] = total_words\n",
    "\n",
    "print('Total sentences before filtering', total_sentences)\n",
    "print('Total words before filtering', total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c02693f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Total sentences after filtering 20481\n",
      "Total words after filtering 152088\n"
     ]
    }
   ],
   "source": [
    "sentece_word_ids = []\n",
    "total_sentences = 0\n",
    "total_words = 0\n",
    "\n",
    "for words in word_list:\n",
    "    filtered_words_ids = [word_to_id[word] for word in words if word in word_to_id.keys()]\n",
    "    words_in_current_sentece = len(filtered_words_ids)\n",
    "    \n",
    "    if words_in_current_sentece >= min_word_count:\n",
    "        total_sentences += 1\n",
    "        total_words += words_in_current_sentece\n",
    "        sentece_word_ids.append(filtered_words_ids)\n",
    "        \n",
    "print('--------------------------------------------')\n",
    "print('Total sentences after filtering', total_sentences)\n",
    "print('Total words after filtering', total_words)\n",
    "\n",
    "data_info['Total sentences after filtering'] = total_sentences\n",
    "data_info['Total words after filtering'] = total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35015a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "del words\n",
    "del word_list\n",
    "del min_word_count                     \n",
    "del word_id  \n",
    "del filtered_words_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211b3a88",
   "metadata": {},
   "source": [
    "## Untill Now \n",
    "<br> \n",
    "<b>sentece_word_ids:</b> Sentence wise word's id. <br>\n",
    "<b>word_to_id:</b> Dictionary for getting the <b>word_id</b> for a <b>word</b>.<br>\n",
    "<b>id_to_word:</b> Dictionary for getting the <b>word</b> for a <b> word_id</b>.<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da05f647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences 20481\n",
      "Total words 152088\n",
      "Total unique words in dictionary 465\n"
     ]
    }
   ],
   "source": [
    "print('Total sentences', total_sentences)\n",
    "print('Total words', total_words)\n",
    "vocabulary_size = len(word_to_id)\n",
    "print('Total unique words in dictionary', vocabulary_size)\n",
    "data_info['Total unique words in dictionary'] = vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4165c",
   "metadata": {},
   "source": [
    "### Loading the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adc2d5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a985fc",
   "metadata": {},
   "source": [
    "### Helper Function for Saving and Loading the Model, Informatino\n",
    "\n",
    "In every cases name will be `skip` for skip-gram model and `cbow` for CBOW model.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abf604e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_name(name):\n",
    "    if name == 'pair':\n",
    "        return 'save/model_pair/'\n",
    "    elif name == 'skip':\n",
    "        return 'save/model_skip/'\n",
    "    elif name == 'cbow':\n",
    "        return 'save/model_cbow/'\n",
    "    \n",
    "def get_unique_file_name():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S-%f\")\n",
    "\n",
    "\n",
    "def save_model(name, model, file_name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    if name == 'cbow':\n",
    "        model.save(path+file_name+'/')\n",
    "    else:\n",
    "         model.save(path+file_name+'.h5')\n",
    "            \n",
    "            \n",
    "def load_model(name, file_name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if name == 'cbow':\n",
    "        return keras.models.load_model(\n",
    "            os.getcwd()+'/'+path+'/'+file_name+'/', custom_objects={\"Average\": Average, \"custom_loss_function\": custom_loss_function}\n",
    "        )\n",
    "    \n",
    "    return tf.keras.models.load_model(path+file_name+'.h5', custom_objects={\"custom_loss_function\": custom_loss_function})\n",
    "\n",
    "\n",
    "def get_model_list(name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        return os.listdir(path)\n",
    "    \n",
    "def save_info(name, file_name, data):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "            \n",
    "    with open(path+file_name+'.yaml', 'w') as file:\n",
    "        documents = yaml.dump(data, file, sort_keys=False)\n",
    "    \n",
    "    \n",
    "def get_info_list(name):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        return os.listdir(path)\n",
    "        \n",
    "\n",
    "def get_info(name, file_name):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "            with open(path+file_name+'.yaml') as file:\n",
    "                return yaml.load(file, Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08e575df",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = get_unique_file_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929921f9",
   "metadata": {},
   "source": [
    "### Generate 2D `vocabulary_size` List for Getting the `one-hot` Vector for a Word Id\n",
    "\n",
    "`one_hot` will work as a `lookup table` to get one-hot encoded vector for a `word_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7eb90fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = np.zeros((vocabulary_size,vocabulary_size))\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    one_hot[index][index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4465d45",
   "metadata": {},
   "source": [
    "# Processing Data for the CBOW and SKIP-GRAM Model\n",
    "\n",
    "In `CBOW` and `SKIP-GRAM` models, will be needed the one-hot encoded `target word` and their corresponding one-hot encoded `context words`. And the size of the one-hot encoded vector will be `vocabulary_size`. \n",
    "\n",
    "One-hot encoded `target_id` and `context_ids` creating process. \n",
    "\n",
    "---<b>target_id: </b>Created a zeros vector of `vocabulary_size`. Then simply assigned the value `one(1)` to an index of the vector, if index number is equal to the `word_id`<br>\n",
    "---<b>context_ids:</b> Created total `window_size * 2`(maximum) `context_ids` for a `word_id`. Then I have created one-hot vector for `context_ids`, using the similar way I have created one-hot vector for a `target_id`. Additionaly, I have also sum up all of the one-hot encoded `context_ids` vectors to a single one-hot encoded vector of size `vocabulary_size`. So, there is one vector of size `vocabulary_size`, for containing all of the `context_ids` of a `target_id`. Because in `CBOW` model, at input layer and in `skip-gram` model at output layer, I have to sum up the one-hot context vectors. Here I have precalcualted it here.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d003cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_contexts(word_ids, window_size):\n",
    "    target_id = []\n",
    "    context_ids = []\n",
    "    \n",
    "    for ids in word_ids:\n",
    "        for index, word_id in enumerate(ids):\n",
    "            if not word_id:\n",
    "                continue\n",
    "                \n",
    "            window_start = max(0, index - window_size)\n",
    "            window_end = min(len(ids), index + window_size + 1)\n",
    "            \n",
    "            target_id.append(one_hot[word_id])\n",
    "            \n",
    "            zero_context_ids = np.zeros(vocabulary_size)\n",
    "            for window_index in range(window_start, window_end):\n",
    "                if window_index != index:\n",
    "                    zero_context_ids[ids[window_index]] += 1\n",
    "            context_ids.append(zero_context_ids)\n",
    "            del zero_context_ids\n",
    "            \n",
    "    return target_id, context_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0901db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "del index                      \n",
    "del min_frquency_of_word       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01d545b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id, context_ids =  get_target_contexts(sentece_word_ids, window_size)\n",
    "data_info['Total target words'] = len(target_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910532d8",
   "metadata": {},
   "source": [
    "### Splitting the Training and Testing Data. Taking 20% Data for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad66e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_target_list, X_test_target_list, Y_train_contexts_list, Y_test_contexts_list = train_test_split(target_id, context_ids, test_size=0.2, random_state=42)\n",
    "del target_id\n",
    "del context_ids\n",
    "Train_target = np.array(X_train_target_list, dtype=np.float16)\n",
    "del X_train_target_list\n",
    "Test_target = np.array(X_test_target_list, dtype=np.float16)\n",
    "del X_test_target_list\n",
    "Train_contexts = np.array(Y_train_contexts_list, dtype=np.float16)\n",
    "del Y_train_contexts_list\n",
    "Test_contexts = np.array(Y_test_contexts_list, dtype=np.float16)\n",
    "del Y_test_contexts_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff14e2",
   "metadata": {},
   "source": [
    "# CBOW Model\n",
    "\n",
    "In `CBOW` model our input will be the vector of `context_ids` to predict the `target_id`. So, `Train_contexts` will be our input and the target will be `Train_target`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a29a8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_cbow =  data_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a9814",
   "metadata": {},
   "source": [
    "### Extending Keras Layer for Taking the Average of Embeddings Layer\n",
    "\n",
    "I have alredys sum up of the input vectors. So, in this layer I've just taking the average after multiplying the the input vector with weight matrix(input layer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d96af11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Average(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Average, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.divide(inputs, window_size*2)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91cd754",
   "metadata": {},
   "source": [
    "### Building the model for CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a15ab51b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 465)]             0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding_cbow (Dense)   (None, 400)               186400    \n",
      "_________________________________________________________________\n",
      "average_1 (Average)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 465)               186465    \n",
      "=================================================================\n",
      "Total params: 372,865\n",
      "Trainable params: 372,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(vocabulary_size, ))\n",
    "x = Dense(400, name='w2v_embedding_cbow')(inp)\n",
    "x = Average(200)(x)\n",
    "x = Dense(vocabulary_size, activation='softmax')(x)\n",
    "\n",
    "model_cbow = Model(inputs=inp, outputs=x)\n",
    "model_cbow.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6990626",
   "metadata": {},
   "source": [
    "### Fitting the CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a971e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  43/1217 [>.............................] - ETA: 4s - loss: 6.0876 - accuracy: 0.0344  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 23:14:15.917878: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217/1217 [==============================] - 4s 3ms/step - loss: 5.8595 - accuracy: 0.0349\n",
      "Epoch 2/100\n",
      "1217/1217 [==============================] - 5s 4ms/step - loss: 5.8135 - accuracy: 0.0355\n",
      "Epoch 3/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.7888 - accuracy: 0.0398\n",
      "Epoch 4/100\n",
      "1217/1217 [==============================] - 5s 4ms/step - loss: 5.7461 - accuracy: 0.0560\n",
      "Epoch 5/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.6967 - accuracy: 0.0628\n",
      "Epoch 6/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.6541 - accuracy: 0.0677\n",
      "Epoch 7/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.6164 - accuracy: 0.0716\n",
      "Epoch 8/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.5805 - accuracy: 0.0755\n",
      "Epoch 9/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.5462 - accuracy: 0.0787\n",
      "Epoch 10/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.5140 - accuracy: 0.0809\n",
      "Epoch 11/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.4836 - accuracy: 0.0832\n",
      "Epoch 12/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.4546 - accuracy: 0.0848\n",
      "Epoch 13/100\n",
      "1217/1217 [==============================] - 5s 4ms/step - loss: 5.4269 - accuracy: 0.0857\n",
      "Epoch 14/100\n",
      "1217/1217 [==============================] - 5s 4ms/step - loss: 5.4009 - accuracy: 0.0869\n",
      "Epoch 15/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.3764 - accuracy: 0.0884\n",
      "Epoch 16/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.3533 - accuracy: 0.0892\n",
      "Epoch 17/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.3315 - accuracy: 0.0906\n",
      "Epoch 18/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.3112 - accuracy: 0.0916\n",
      "Epoch 19/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.2919 - accuracy: 0.0927\n",
      "Epoch 20/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.2737 - accuracy: 0.0944\n",
      "Epoch 21/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.2564 - accuracy: 0.0957\n",
      "Epoch 22/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.2397 - accuracy: 0.0968\n",
      "Epoch 23/100\n",
      "1217/1217 [==============================] - 5s 4ms/step - loss: 5.2238 - accuracy: 0.0977\n",
      "Epoch 24/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.2086 - accuracy: 0.0986\n",
      "Epoch 25/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.1938 - accuracy: 0.0996\n",
      "Epoch 26/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.1795 - accuracy: 0.1006\n",
      "Epoch 27/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.1659 - accuracy: 0.1013\n",
      "Epoch 28/100\n",
      "1217/1217 [==============================] - 4s 4ms/step - loss: 5.1525 - accuracy: 0.1024\n",
      "Epoch 29/100\n",
      " 441/1217 [=========>....................] - ETA: 2s - loss: 5.1345 - accuracy: 0.1041"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6_/xv_wk6qn5k9g5yztczrn0zlh0000gn/T/ipykernel_986/796103233.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtraining_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_cbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_loss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhistory_cbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_contexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def custom_loss_function(y_true, y_pred):\n",
    "    return tf.reduce_mean(-tf.math.reduce_sum(y_true * tf.math.log(y_pred), axis=[1]))\n",
    "\n",
    "\n",
    "training_started = datetime.datetime.now()\n",
    "model_cbow.compile(loss=custom_loss_function, optimizer=tf.optimizers.SGD(learning_rate=0.9), metrics=['accuracy'])\n",
    "history_cbow = model_cbow.fit(Train_contexts, Train_target, epochs = 100, batch_size=100, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac711ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end = datetime.datetime.now()\n",
    "data_info_cbow['training time'] =  training_end - training_started\n",
    "data_info_cbow['cbow model training history'] = history_cbow.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5237b",
   "metadata": {},
   "source": [
    "### Saving CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e8ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_model('cbow', model_cbow, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439e276",
   "metadata": {},
   "source": [
    "### Testing CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b76ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_cbow.evaluate(Test_contexts, Test_target, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_cbow['cbow model evaluatation loss'] = loss\n",
    "data_info_cbow['cbow model evaluatation accuracy'] = accuracy\n",
    "print('Evaluated loss',loss, 'and accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f931e73",
   "metadata": {},
   "source": [
    "### Saving Information for CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b359bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info('cbow',file_name, data_info_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cf891",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_cbow = model_cbow.get_layer('w2v_embedding_cbow').get_weights()[0]\n",
    "\n",
    "path_cbow = get_path_name('cbow') + 'vector_metadata/'\n",
    "\n",
    "\n",
    "if not os.path.exists(path_cbow):\n",
    "        os.makedirs(path_cbow)\n",
    "\n",
    "out_v = io.open(path_cbow+file_name+'vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open(path_cbow+file_name+'metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    vec = weights_cbow[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(id_to_word[index] + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542756c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('End of CBOW model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del accuracy                   \n",
    "del custom_loss_function       \n",
    "del data_info_cbow            \n",
    "del history_cbow              \n",
    "del index                      \n",
    "del inp                       \n",
    "del model_cbow                 \n",
    "del out_m                      \n",
    "del out_v                     \n",
    "del path_cbow                  \n",
    "del train_test_split          \n",
    "del training_end              \n",
    "del training_started           \n",
    "del vec                        \n",
    "del weights_cbow               \n",
    "del words_in_current_sentece   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db2008",
   "metadata": {},
   "source": [
    "## SKIP-GRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_skip = data_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a63d3f",
   "metadata": {},
   "source": [
    "### Creating SKIP-GRAMS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a285d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(vocabulary_size, ))\n",
    "x = Dense(500, name='w2v_embedding_skip', bias_initializer=tf.initializers.RandomNormal(stddev=1.0))(inp)\n",
    "x = Dense(vocabulary_size, bias_initializer=tf.initializers.RandomNormal(stddev=1.0), activation='softmax')(x)\n",
    "model_skip = Model(inputs=inp, outputs=x)\n",
    "model_skip.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464c747",
   "metadata": {},
   "source": [
    "### Fitting Skip-Grams Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_started = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_function(y_true, y_pred):\n",
    "    return tf.reduce_mean(-tf.math.reduce_sum(y_true * tf.math.log(y_pred), axis=[1]))\n",
    "\n",
    "\n",
    "model_skip.compile(loss=custom_loss_function, optimizer=tf.optimizers.SGD(learning_rate=0.01), metrics=['accuracy'])\n",
    "history_skip = model_skip.fit(Train_target, Train_contexts, epochs = 40, batch_size=100, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a04383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_function(y_true, y_pred):\n",
    "    return tf.reduce_mean(-tf.math.reduce_sum(y_true * tf.math.log(y_pred), axis=[1]))\n",
    "\n",
    "\n",
    "model_skip.compile(loss=custom_loss_function, optimizer=tf.optimizers.SGD(learning_rate=0.01), metrics=['accuracy'])\n",
    "history_skip = model_skip.fit(Train_target, Train_contexts, epochs = 5, batch_size=2, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43454535",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76fbef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end = datetime.datetime.now()\n",
    "data_info_skip['training time'] =  training_end - training_started\n",
    "data_info_skip['skip-grams model training history'] = history_skip.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddba413c",
   "metadata": {},
   "source": [
    "### Saving SKIP_GRAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b192848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('skip', model_skip, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e86d1",
   "metadata": {},
   "source": [
    "### Testing SKIP_GRAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc896b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_skip.evaluate(Test_target, Test_contexts, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99f4e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_info_skip['skip model evaluatation loss'] = loss\n",
    "data_info_skip['skip model evaluatation accuracy'] = accuracy\n",
    "print('Evaluated loss',loss, 'and accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc45cf",
   "metadata": {},
   "source": [
    "### Saving Information for Skip-Gram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6674ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info('skip',file_name, data_info_skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71849642",
   "metadata": {},
   "source": [
    "### Saving Vector and Metadata For SKIP-GRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_skip = model_skip.get_layer('w2v_embedding_skip').get_weights()[0]\n",
    "\n",
    "path_skip = get_path_name('skip') + 'vector_metadata/'\n",
    "\n",
    "\n",
    "if not os.path.exists(path_skip):\n",
    "        os.makedirs(path_skip)\n",
    "\n",
    "out_v = io.open(path_skip+file_name+'vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open(path_skip+file_name+'metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    vec = weights_skip[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(id_to_word[index] + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f0ef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('End Skip-Gram Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad6c755",
   "metadata": {},
   "source": [
    "# Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773814e5",
   "metadata": {},
   "source": [
    "### CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450cbc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cbow = load_model('cbow', file_name)\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ab5cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_info('cbow', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae11c3",
   "metadata": {},
   "source": [
    "### Skip-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19878f7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "skip = load_model('skip', file_name)\n",
    "print(skip.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0facc9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_info('skip', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988dba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('THE END')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflowEnv] *",
   "language": "python",
   "name": "conda-env-tensorflowEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

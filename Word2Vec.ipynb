{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80387ad",
   "metadata": {},
   "source": [
    "### Iimport libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0113bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import yaml\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Dropout\n",
    "from keras.preprocessing import text\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import losses, optimizers\n",
    "from tensorflow.keras.activations import softmax\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fdfc9",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c09c6622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Head------------------\n",
      "   ARTICLE_ID      TITLE                 SECTION_TITLE  \\\n",
      "0           0  Anarchism                  Introduction   \n",
      "1           0  Anarchism     Etymology and terminology   \n",
      "2           0  Anarchism                       History   \n",
      "3           0  Anarchism  Anarchist schools of thought   \n",
      "4           0  Anarchism   Internal issues and debates   \n",
      "\n",
      "                                        SECTION_TEXT  \n",
      "0  \\n\\n\\n\\n\\n\\n'''Anarchism''' is a political phi...  \n",
      "1  \\n\\nThe term ''anarchism'' is a compound word ...  \n",
      "2  \\n\\n===Origins===\\nWoodcut from a Diggers docu...  \n",
      "3  \\nPortrait of philosopher Pierre-Joseph Proudh...  \n",
      "4  \\nconsistent with anarchist values is a contro...  \n",
      "------------------Tail------------------\n",
      "        ARTICLE_ID              TITLE    SECTION_TITLE  \\\n",
      "265134       30475  Triboluminescence  Further reading   \n",
      "265135       30475  Triboluminescence   External links   \n",
      "265136       30476       Markov chain     Introduction   \n",
      "265137       30476       Markov chain     Introduction   \n",
      "265138       30476       Markov chain          History   \n",
      "\n",
      "                                             SECTION_TEXT  \n",
      "265134                                           * \\n* \\n  \n",
      "265135  \\n* \\n* \\n*  Triboluminescence Discussion on T...  \n",
      "265136  \\nIn probability theory and related fields, a ...  \n",
      "265137  Russian mathematician Andrey Markov.\\nA Markov...  \n",
      "265138  Andrey Markov studied Markov chains in the ear...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('enwiki.csv')\n",
    "print('------------------Head------------------')\n",
    "print(df.head())\n",
    "print('------------------Tail------------------')\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f55548",
   "metadata": {},
   "source": [
    "### Remove unnecessary data from dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795dbd54",
   "metadata": {},
   "source": [
    "# Set constrains for filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db94d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc946361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 0 for taking all data\n",
    "data_limit = 300 \n",
    "max_sentence_lenght = 1000\n",
    "max_word_count = 100\n",
    "min_word_count = 5\n",
    "max_sentence = 1000\n",
    "vocabulary_size = 30\n",
    "window_size = 4\n",
    "\n",
    "\n",
    "data_info['data_limit']= data_limit\n",
    "data_info['max_sentence_lenght'] = max_sentence_lenght\n",
    "data_info['max_word_count'] = max_word_count \n",
    "data_info['min_word_count'] = min_word_count\n",
    "data_info['max_sentence'] = max_sentence \n",
    "data_info['vocabulary_size'] = vocabulary_size\n",
    "data_info['window_size'] = window_size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78849ce5",
   "metadata": {},
   "source": [
    "### Create a dictioanry for saving the data related information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a027071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total section texts -----> 265139\n"
     ]
    }
   ],
   "source": [
    "df.drop(['ARTICLE_ID', 'TITLE', 'SECTION_TITLE'], axis=1)\n",
    "section_texts = df['SECTION_TEXT'].apply(str)\n",
    "print('Total section texts ----->', len(section_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f4f6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total section texts -----> 300\n"
     ]
    }
   ],
   "source": [
    "if data_limit != 0:\n",
    "    section_texts = section_texts[:data_limit]\n",
    "print('Total section texts ----->', len(section_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7340200",
   "metadata": {},
   "source": [
    "### Convert text to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c779773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for texts in section_texts:\n",
    "    for sentence in sent_tokenize(texts):\n",
    "        if len(sentence) < max_sentence_lenght:\n",
    "            sentences.append(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a62ec2df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences 6661\n"
     ]
    }
   ],
   "source": [
    "total_sentences = len(sentences)\n",
    "print('Total sentences', total_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d56d9",
   "metadata": {},
   "source": [
    "### Convert sentences to words and create vocabulary with frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c794f606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "word_list = []\n",
    "vocabulary_with_frequency = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    words_without_stop_words = [word for word in words if word.isalpha() and word not in stop_words and len(word) != 1]\n",
    "    \n",
    "    word_lenght = len(words_without_stop_words)\n",
    "    if word_lenght <= max_word_count and word_lenght >= min_word_count:\n",
    "        word_list.append(words_without_stop_words)\n",
    "        \n",
    "        for word in words_without_stop_words:\n",
    "            if word not in vocabulary_with_frequency.keys():\n",
    "                vocabulary_with_frequency[word] = 1\n",
    "            else:\n",
    "                vocabulary_with_frequency[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015c0bd",
   "metadata": {},
   "source": [
    "### Total words and vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c459fec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in corpus 80055\n",
      "Vocabulary size 15960\n"
     ]
    }
   ],
   "source": [
    "# After filtering the words\n",
    "total_sentences = len(word_list)\n",
    "\n",
    "total_words = 0\n",
    "for words in word_list:\n",
    "    total_words += len(words)\n",
    "print('Total words in corpus', total_words)\n",
    "print('Vocabulary size', len(vocabulary_with_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebb1bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocabulary_with_frequency = sorted(vocabulary_with_frequency.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b6cc6",
   "metadata": {},
   "source": [
    "### Remove less freuquent word form dictionary and assign unique id to vocabulary word\n",
    "\n",
    "\n",
    "<br> \n",
    "Create two vocabulary.<br> \n",
    "<b>word_to_id</b> for gettig the word for an id. <br>\n",
    "<b>id_to_word</b> for gettign the id for word. <br>\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2588df39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'PAD'), ('lincoln', 1), ('also', 2), ('one', 3), ('apollo', 4), ('state', 5), ('first', 6), ('achilles', 7), ('alabama', 8), ('aristotle', 9), ('time', 10), ('autism', 11), ('many', 12), ('new', 13), ('century', 14)]\n",
      "-------------------------------------\n",
      "[('PAD', 0), (1, 'lincoln'), (2, 'also'), (3, 'one'), (4, 'apollo'), (5, 'state'), (6, 'first'), (7, 'achilles'), (8, 'alabama'), (9, 'aristotle'), (10, 'time'), (11, 'autism'), (12, 'many'), (13, 'new'), (14, 'century')]\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {}\n",
    "word_to_id[0] = 'PAD'\n",
    "id_to_word = {}\n",
    "id_to_word['PAD'] = 0\n",
    "word_id = 1\n",
    "\n",
    "for word, _ in sorted_vocabulary_with_frequency:\n",
    "    if word_id < vocabulary_size:\n",
    "        word_to_id[word] = word_id\n",
    "        id_to_word[word_id] = word\n",
    "        word_id += 1\n",
    "        \n",
    "print(list(islice(word_to_id.items(), 15)))\n",
    "print('-------------------------------------')\n",
    "print(list(islice(id_to_word.items(), 15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92629475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dictionary size 30\n"
     ]
    }
   ],
   "source": [
    "print('New dictionary size', vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3b870",
   "metadata": {},
   "source": [
    "### Convert word_list to word_id_list for expressign the words of a sentece using vocabulary id\n",
    "\n",
    "<br> Remove the words which are not present in dictionary</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcb0dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info['total sentence'] = total_sentences\n",
    "data_info['total words'] = total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c02693f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering total sentenc 6087\n",
      "Before filtering total words 80055\n",
      "--------------------------------------------\n",
      "After filtering total sentences 82\n",
      "After filtering total words 486\n"
     ]
    }
   ],
   "source": [
    "print('Before filtering total sentenc', total_sentences)\n",
    "print('Before filtering total words', total_words)\n",
    "\n",
    "sentece_word_ids = []\n",
    "total_sentences = 0\n",
    "total_words = 0\n",
    "\n",
    "for words in word_list:\n",
    "    filtered_words_ids = [word_to_id[word] for word in words if word in word_to_id.keys()]\n",
    "    words_in_current_sentece = len(filtered_words_ids)\n",
    "    \n",
    "    if words_in_current_sentece >= min_word_count:\n",
    "        total_sentences += 1\n",
    "        total_words += words_in_current_sentece\n",
    "        sentece_word_ids.append(filtered_words_ids)\n",
    "        \n",
    "print('--------------------------------------------')\n",
    "print('After filtering total sentences', total_sentences)\n",
    "print('After filtering total words', total_words)\n",
    "\n",
    "data_info['After filtering total sentencs'] = total_sentences\n",
    "data_info['After filtering total words'] = total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211b3a88",
   "metadata": {},
   "source": [
    "### Untill Now \n",
    "<br> \n",
    "<b>sentece_word_ids:</b> Sentence wise words's id. <br>\n",
    "<b>word_to_id:</b> Vocabulary for getting the <b>word_id</b> for a <b>word</b>.<br>\n",
    "<b>id_to_word:</b> Vocabulary for getting the <b>word</b> for a <b> word_id</b>.<br>\n",
    "<b>positive_skip_grams:</b> skipgrams of target and context word pairs.\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da05f647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total senteces 82\n",
      "Total words 486\n",
      "Total unique words in vocabulary 30\n"
     ]
    }
   ],
   "source": [
    "print('Total senteces', total_sentences)\n",
    "print('Total words', total_words)\n",
    "vocabulary_size = len(word_to_id)\n",
    "print('Total unique words in vocabulary', vocabulary_size)\n",
    "\n",
    "data_info['total_unique words in vocabulary'] = vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bedcf3",
   "metadata": {},
   "source": [
    "### Save data for later use\n",
    "<br> It will help use to skip the data processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78be2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_data(data, to_file):\n",
    "#     if os.path.exists(to_file+'.yaml'):\n",
    "#         os.remove(to_file+'.yaml')\n",
    "    \n",
    "#     with open(to_file+'.yaml', 'w') as file:\n",
    "#         documents = yaml.dump(data, file, sort_keys=False)\n",
    "    \n",
    "# def load_data(from_file):\n",
    "#     with open(from_file+'.yaml') as file:\n",
    "#         return yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7c7fc",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0f86fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_data(word_to_id, 'word_to_id')\n",
    "# save_data(id_to_word, 'id_to_word')\n",
    "# save_data(sentece_word_ids, 'sentece_word_ids')\n",
    "# # save_data(positive_skip_grams, 'positive_skip_grams')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca271ed1",
   "metadata": {},
   "source": [
    "# ⚠️⚠️⚠️ Reset Everything ⚠️⚠️⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dd4a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8e0a5",
   "metadata": {},
   "source": [
    "# Start again 🏃‍♂️🏃‍♂️🏃‍♂️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ee5f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import sys\n",
    "# import yaml\n",
    "# import os\n",
    "# import io\n",
    "\n",
    "# import nltk\n",
    "# from nltk.corpus import PlaintextCorpusReader\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# from tensorflow.keras import layers\n",
    "# from keras.preprocessing import text\n",
    "\n",
    "# from itertools import islice\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from tensorflow.keras.layers import Embedding, Input, Dense, Dropout\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras import losses, optimizers\n",
    "# from tensorflow.keras.activations import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbfdc047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_data(data, to_file):\n",
    "#     if os.path.exists(to_file+'.yaml'):\n",
    "#         os.remove('id_to_word.yaml')\n",
    "    \n",
    "#     with open(to_file+'.yaml', 'w') as file:\n",
    "#         documents = yaml.dump(data, file, sort_keys=False)\n",
    "    \n",
    "# def load_data(from_file):\n",
    "#     with open(from_file+'.yaml') as file:\n",
    "#         return yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b374e",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1927780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_to_word = load_data('id_to_word')\n",
    "# word_to_id = load_data('word_to_id')\n",
    "# sentece_word_ids = load_data('sentece_word_ids')\n",
    "# # positive_skip_grams = load_data('positive_skip_grams')\n",
    "# vocabulary_size = len(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1005f",
   "metadata": {},
   "source": [
    "#### Example after loadign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec6d81a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PAD', 0), (1, 'lincoln'), (2, 'also'), (3, 'one'), (4, 'apollo'), (5, 'state'), (6, 'first'), (7, 'achilles'), (8, 'alabama'), (9, 'aristotle')]\n",
      "------------------------------------------\n",
      "[(0, 'PAD'), ('lincoln', 1), ('also', 2), ('one', 3), ('apollo', 4), ('state', 5), ('first', 6), ('achilles', 7), ('alabama', 8), ('aristotle', 9)]\n",
      "------------------------------------------\n",
      "[[16, 12, 26, 14, 24, 24], [5, 24, 19, 5, 6, 19], [24, 12, 24, 25, 24], [3, 24, 6, 24, 10], [6, 29, 24, 6, 24]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(list(islice(id_to_word.items(), 10)))\n",
    "print('------------------------------------------')\n",
    "print(list(islice(word_to_id.items(), 10)))\n",
    "print('------------------------------------------')\n",
    "print(sentece_word_ids[:5])\n",
    "print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adc2d5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929921f9",
   "metadata": {},
   "source": [
    "### Generate 2D `vocabulary_size` list for getting the one-hot vector for a index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d093fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = [[0 for i in range(vocabulary_size)] for j in range(vocabulary_size)]\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    one_hot[index][index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a985fc",
   "metadata": {},
   "source": [
    "### Helper Function\n",
    "\n",
    "In every cases name will be `pari`, `skip` or `cbow`.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abf604e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_name(name):\n",
    "    if name == 'pair':\n",
    "        return 'save/model_pair/'\n",
    "    elif name == 'skip':\n",
    "        return 'save/model_skip/'\n",
    "    elif name == 'cbow':\n",
    "        return 'save/model_cbow/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4f252cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_file_name():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S-%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d74957e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(name, model, file_name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    if name == 'cbow':\n",
    "        model.save(path+file_name+'/')\n",
    "    else:\n",
    "         model.save(path+file_name+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ef6f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name, file_name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if name == 'cbow':\n",
    "        print(os.getcwd()+path+file_name)\n",
    "        return keras.models.load_model(\n",
    "            os.getcwd()+'/'+path+'/'+file_name+'/', custom_objects={\"Average\": Average}\n",
    "        )\n",
    "    \n",
    "    return tf.keras.models.load_model(path+file_name+'.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0193aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_list(name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        return os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c21e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_info(name, file_name, data):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "            \n",
    "    with open(path+file_name+'.yaml', 'w') as file:\n",
    "        documents = yaml.dump(data, file, sort_keys=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a79ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_list(name):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        return os.listdir(path)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "295286d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(name, file_name):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "            with open(path+file_name+'.yaml') as file:\n",
    "                return yaml.load(file, Loader=yaml.Loader)\n",
    "            \n",
    "#             yaml.load('Foo: !Ref bar', Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08e575df",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = get_unique_file_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de79e51",
   "metadata": {},
   "source": [
    "# `Model - Pair Input`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e14b798",
   "metadata": {},
   "source": [
    "### Save data_info to data_info_pair.\n",
    "<br> Because `data_info` is common for all model</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "170d39e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_pair = data_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e3e0dd",
   "metadata": {},
   "source": [
    "### Create (taget, context) pair for all of the words and convert them into one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9aea7804",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2248, 30)\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for ids in sentece_word_ids:\n",
    "    skip, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "        ids,\n",
    "        vocabulary_size,\n",
    "        window_size=window_size, negative_samples=0,\n",
    "        shuffle=False,\n",
    "        categorical=True)\n",
    "    for target, context in skip:\n",
    "        X.append(one_hot[target])\n",
    "        Y.append(one_hot[context])\n",
    "print(np.array(X).shape)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3f72ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_pair['Total target and context word pairs'] = len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a2d480",
   "metadata": {},
   "source": [
    "### Split training testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c54b3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pair, X_test_pair, Y_train_pair, Y_test_pair = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858d9a7d",
   "metadata": {},
   "source": [
    "### Pairwise target, context model\n",
    "<br>\n",
    "Input target word in one-hot encoding format. <br>\n",
    "Output context word of corespoding target word in one-hot encoding format. <br>\n",
    "\n",
    "All of the targets and their corresponding context words are fitting at once. \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d52257e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding_pair (Dense)   (None, 200)               6200      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 12,230\n",
      "Trainable params: 12,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 03:17:44.848953: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-11-19 03:17:44.849048: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(vocabulary_size, ))\n",
    "x = Dense(200, name='w2v_embedding_pair')(inp)\n",
    "x = Dense(vocabulary_size, activation='softmax')(x)\n",
    "\n",
    "model_pair = Model(inputs=inp, outputs=x)\n",
    "model_pair.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a227a1e3",
   "metadata": {},
   "source": [
    "### Fite the Pair model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2befd208",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_started = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b298f364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 03:17:45.084152: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-11-19 03:17:45.084344: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2021-11-19 03:17:45.247742: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "899/899 [==============================] - 4s 4ms/step - loss: 2.8017 - accuracy: 0.3009\n",
      "Epoch 2/3\n",
      "899/899 [==============================] - 4s 4ms/step - loss: 2.4041 - accuracy: 0.3432\n",
      "Epoch 3/3\n",
      "899/899 [==============================] - 4s 5ms/step - loss: 2.3332 - accuracy: 0.3426\n"
     ]
    }
   ],
   "source": [
    "model_pair.compile(loss=losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "history_pair = model_pair.fit(X_train_pair, Y_train_pair, epochs = 3, batch_size=2, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee9db39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end = datetime.datetime.now()\n",
    "data_info_pair['training time'] =  training_end - training_started\n",
    "data_info_pair['pair model training history'] = history_pair.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7603684",
   "metadata": {},
   "source": [
    "### Save Pair Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd98a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('pair', model_pair, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f221c91e",
   "metadata": {},
   "source": [
    "### Testing Pair Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "415534c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 - 0s - loss: 2.5585 - accuracy: 0.3133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 03:17:57.400515: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_pair.evaluate(X_test_pair, Y_test_pair, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b57bfc34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated loss 2.558540105819702 and accuracy 0.31333333253860474\n"
     ]
    }
   ],
   "source": [
    "data_info_pair['pair model evaluatation loss'] = loss\n",
    "data_info_pair['pair model evaluatation accuracy'] = accuracy\n",
    "print('Evaluated loss',loss, 'and accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc5387",
   "metadata": {},
   "source": [
    "### Save Pair Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4dd49408",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info('pair',file_name, data_info_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61167fd7",
   "metadata": {},
   "source": [
    "### Store the vector and metadata for Pair Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00715211",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model_pair.get_layer('w2v_embedding_pair').get_weights()[0]\n",
    "\n",
    "path = get_path_name('pair') + 'vector_metadata/'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "out_v = io.open(path+file_name+'vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open(path+file_name+'metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(id_to_word[index] + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0636d03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Pair Model\n"
     ]
    }
   ],
   "source": [
    "print('End Pair Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4465d45",
   "metadata": {},
   "source": [
    "# Data Processing for the CBOW and SKIP-GRAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfe7554",
   "metadata": {},
   "source": [
    "<br> This function creates the target words and all of it's corrosponding context words.<br>\n",
    "return:<br>\n",
    "------<b>target_id:</b> id of the target word without encoding.<br>\n",
    "------<b>context_ids:</b> All of the context words for a target_id<br>\n",
    "\n",
    "<b>Note:</b> Zero padding is being used for returing total `window_size * 2` context words for a target word.<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d003cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_target_contexts(word_ids, window_size):\n",
    "    target_id = []\n",
    "    context_ids = []\n",
    "\n",
    "    one_hot_target_id = []\n",
    "    one_hot_context_ids = []\n",
    "\n",
    "    for ids in word_ids:\n",
    "        for index, word_id in enumerate(ids):\n",
    "            if not word_id:\n",
    "                continue\n",
    "                \n",
    "            window_start = max(0, index - window_size)\n",
    "            window_end = min(len(ids), index + window_size + 1)\n",
    "            \n",
    "            target_id.append(word_id)\n",
    "            context_ids.append([ids[window_index] for window_index in range(window_start, window_end) if window_index != index])\n",
    "                    \n",
    "    return target_id, pad_sequences(context_ids, maxlen = window_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01d545b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id, context_ids =  get_target_contexts(sentece_word_ids, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8ea2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info['Target words'] = len(target_id)\n",
    "data_info['Context words with zero padding'] = len(context_ids) * window_size * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f3453c",
   "metadata": {},
   "source": [
    "### Express `target_id` &  `context_ids` using one-hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26160bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_target_id = [one_hot[id] for id in target_id]\n",
    "one_hot_context_ids = []\n",
    "\n",
    "for context_id in context_ids:\n",
    "    one_hot_context_ids.append([one_hot[id] for id in context_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806fd6a1",
   "metadata": {},
   "source": [
    "### Take the sum of input vecotor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7bdb3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for context in one_hot_context_ids:\n",
    "    row = [0] * vocabulary_size\n",
    "    for word in context:\n",
    "        for idx in range(vocabulary_size):\n",
    "            row[idx] += word[idx]\n",
    "    X.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff14e2",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cdd5d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_cbow =  data_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a09cc4",
   "metadata": {},
   "source": [
    "### Split training testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad66e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cbow, X_test_cbow, Y_train_cbow, Y_test_cbow = train_test_split(X, one_hot_target_id, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44c15a0",
   "metadata": {},
   "source": [
    "#### Extending keras layer for taking the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d96af11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Average(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Average, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.divide(inputs, window_size*2)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a15ab51b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding_cbow (Dense)   (None, 200)               6200      \n",
      "_________________________________________________________________\n",
      "average (Average)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 12,230\n",
      "Trainable params: 12,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(vocabulary_size, ))\n",
    "x = Dense(200, name='w2v_embedding_cbow')(inp)\n",
    "x = Average(200)(x)\n",
    "x = Dense(vocabulary_size, activation='softmax')(x)\n",
    "\n",
    "model_cbow = Model(inputs=inp, outputs=x)\n",
    "model_cbow.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87a729c",
   "metadata": {},
   "source": [
    "### Fit the CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1b5f942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_started = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a971e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.3940 - accuracy: 0.0412\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3482 - accuracy: 0.2139\n",
      "Epoch 3/3\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 3.3258 - accuracy: 0.2600"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 03:17:57.698100: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3044 - accuracy: 0.3093\n"
     ]
    }
   ],
   "source": [
    "model_cbow.compile(loss=losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "history_cbow = model_cbow.fit(X_train_cbow, Y_train_cbow, epochs = 3, batch_size=50, verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ac711ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end = datetime.datetime.now()\n",
    "data_info_cbow['training time'] =  training_end - training_started\n",
    "data_info_cbow['cbow model training history'] = history_cbow.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5237b",
   "metadata": {},
   "source": [
    "### Save CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b33e8ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: save/model_cbow/model/2021-11-19_03-17-44-823361/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 03:17:57.973651: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "save_model('cbow', model_pair, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439e276",
   "metadata": {},
   "source": [
    "### Testing CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72b76ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 03:17:58.145453: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 0s - loss: 3.3266 - accuracy: 0.1939\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_cbow.evaluate(X_test_cbow, Y_test_cbow, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c8c23872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated loss 3.326587438583374 and accuracy 0.19387754797935486\n"
     ]
    }
   ],
   "source": [
    "data_info_cbow['cbow model evaluatation loss'] = loss\n",
    "data_info_cbow['cbow model evaluatation accuracy'] = accuracy\n",
    "print('Evaluated loss',loss, 'and accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0670712",
   "metadata": {},
   "source": [
    "### Save CBOW model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bf95868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info('cbow',file_name, data_info_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "43ed1c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of CBOW model\n"
     ]
    }
   ],
   "source": [
    "print('End of CBOW model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db2008",
   "metadata": {},
   "source": [
    "## SKIP-GRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d43c9944",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_skip = data_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8571e1",
   "metadata": {},
   "source": [
    "### Express the context ids(window_size *2) of a targetd id into a single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ceec85e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_one_hot_context_ids = [sum(one_hot_context_id, []) for one_hot_context_id in one_hot_context_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adb3b23",
   "metadata": {},
   "source": [
    "### Split training testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92201473",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_skip, X_test_skip, Y_train_skip, Y_test_skip = train_test_split(one_hot_target_id, flatten_one_hot_context_ids, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1610bc4",
   "metadata": {},
   "source": [
    "### Create SKIP-GRAMS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a0a285d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding_skip (Dense)   (None, 200)               6200      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 240)               48240     \n",
      "=================================================================\n",
      "Total params: 54,440\n",
      "Trainable params: 54,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(vocabulary_size, ))\n",
    "x = Dense(200, name='w2v_embedding_skip')(inp)\n",
    "x = Dense(vocabulary_size * window_size * 2, activation='softmax')(x)\n",
    "model_skip = Model(inputs=inp, outputs=x)\n",
    "model_skip.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0fd852",
   "metadata": {},
   "source": [
    "### Fit skip-grams model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea78c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_started = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f199d529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 25.0510 - accuracy: 0.0129\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 24.7265 - accuracy: 0.0309\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 24.3923 - accuracy: 0.0335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 03:17:58.495028: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 24.0087 - accuracy: 0.0309\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 23.5601 - accuracy: 0.0309\n"
     ]
    }
   ],
   "source": [
    "model_skip.compile(loss=losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "history_skip = model_skip.fit(X_train_skip, Y_train_skip, epochs = 5, batch_size=50, verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d64a2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end = datetime.datetime.now()\n",
    "data_info_skip['training time'] =  training_end - training_started\n",
    "data_info_skip['skip-grams model training history'] = history_skip.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012beb5",
   "metadata": {},
   "source": [
    "### Save SKIP_GRAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b514b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('skip', model_skip, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c16e8f",
   "metadata": {},
   "source": [
    "### Test SKIP_GRAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2cc896b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 0s - loss: 24.8106 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 03:17:58.901534: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_skip.evaluate(X_test_skip, Y_test_skip, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a947b6b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated loss 24.81056785583496 and accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "data_info_skip['skip model evaluatation loss'] = loss\n",
    "data_info_skip['skip model evaluatation accuracy'] = accuracy\n",
    "print('Evaluated loss',loss, 'and accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c711b",
   "metadata": {},
   "source": [
    "### Data info for Skip-Grams model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9bec88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info('skip',file_name, data_info_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1ece2eee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Skip-Gram Model\n"
     ]
    }
   ],
   "source": [
    "print('End Skip-Gram Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213427cd",
   "metadata": {},
   "source": [
    "# Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e2813",
   "metadata": {},
   "source": [
    "### Pair wise input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c8d2451a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding_pair (Dense)   (None, 200)               6200      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 12,230\n",
      "Trainable params: 12,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pair = load_model('pair', file_name)\n",
    "print(pair.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cc26466e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_limit': 300,\n",
       " 'max_sentence_lenght': 1000,\n",
       " 'max_word_count': 100,\n",
       " 'min_word_count': 5,\n",
       " 'max_sentence': 1000,\n",
       " 'vocabulary_size': 30,\n",
       " 'window_size': 4,\n",
       " 'total sentence': 6087,\n",
       " 'total words': 80055,\n",
       " 'After filtering total sentencs': 82,\n",
       " 'After filtering total words': 486,\n",
       " 'total_unique words in vocabulary': 30,\n",
       " 'Total target and context word pairs': 2248,\n",
       " 'training time': datetime.timedelta(seconds=12, microseconds=409463),\n",
       " 'pair model training history': {'loss': [2.801678419113159,\n",
       "   2.404081344604492,\n",
       "   2.3332059383392334],\n",
       "  'accuracy': [0.30088990926742554, 0.3431590795516968, 0.3426029086112976]},\n",
       " 'pair model evaluatation loss': 2.558540105819702,\n",
       " 'pair model evaluatation accuracy': 0.31333333253860474}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_info('pair', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd753687",
   "metadata": {},
   "source": [
    "### CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f4124535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ruman/Documents/project/word2Vec/word2vecsave/model_cbow/model/2021-11-19_03-17-44-823361\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding_pair (Dense)   (None, 200)               6200      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 12,230\n",
      "Trainable params: 12,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cbow = load_model('cbow', file_name)\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c4d31836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_limit': 300,\n",
       " 'max_sentence_lenght': 1000,\n",
       " 'max_word_count': 100,\n",
       " 'min_word_count': 5,\n",
       " 'max_sentence': 1000,\n",
       " 'vocabulary_size': 30,\n",
       " 'window_size': 4,\n",
       " 'total sentence': 6087,\n",
       " 'total words': 80055,\n",
       " 'After filtering total sentencs': 82,\n",
       " 'After filtering total words': 486,\n",
       " 'total_unique words in vocabulary': 30,\n",
       " 'Target words': 486,\n",
       " 'Context words with zero padding': 3888,\n",
       " 'training time': datetime.timedelta(microseconds=343414),\n",
       " 'cbow model training history': {'loss': [3.3940224647521973,\n",
       "   3.3482391834259033,\n",
       "   3.3044378757476807],\n",
       "  'accuracy': [0.04123711585998535, 0.213917538523674, 0.30927836894989014]},\n",
       " 'cbow model evaluatation loss': 3.326587438583374,\n",
       " 'cbow model evaluatation accuracy': 0.19387754797935486}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_info('cbow', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e131e6",
   "metadata": {},
   "source": [
    "### Skip-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f2e62974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding_skip (Dense)   (None, 200)               6200      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 240)               48240     \n",
      "=================================================================\n",
      "Total params: 54,440\n",
      "Trainable params: 54,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "skip = load_model('skip', file_name)\n",
    "print(skip.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cafe0cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_limit': 300,\n",
       " 'max_sentence_lenght': 1000,\n",
       " 'max_word_count': 100,\n",
       " 'min_word_count': 5,\n",
       " 'max_sentence': 1000,\n",
       " 'vocabulary_size': 30,\n",
       " 'window_size': 4,\n",
       " 'total sentence': 6087,\n",
       " 'total words': 80055,\n",
       " 'After filtering total sentencs': 82,\n",
       " 'After filtering total words': 486,\n",
       " 'total_unique words in vocabulary': 30,\n",
       " 'Target words': 486,\n",
       " 'Context words with zero padding': 3888,\n",
       " 'training time': datetime.timedelta(microseconds=574650),\n",
       " 'skip-grams model training history': {'loss': [25.050983428955078,\n",
       "   24.726524353027344,\n",
       "   24.39234733581543,\n",
       "   24.008689880371094,\n",
       "   23.560100555419922],\n",
       "  'accuracy': [0.012886598706245422,\n",
       "   0.030927836894989014,\n",
       "   0.0335051566362381,\n",
       "   0.030927836894989014,\n",
       "   0.030927836894989014]},\n",
       " 'skip model evaluatation loss': 24.81056785583496,\n",
       " 'skip model evaluatation accuracy': 0.0}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_info('skip', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "74ce7c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average\t Dense\t Dropout\t Embedding\t Input\t Model\t PlaintextCorpusReader\t X\t X_test_cbow\t \n",
      "X_test_pair\t X_test_skip\t X_train_cbow\t X_train_pair\t X_train_skip\t Y\t Y_test_cbow\t Y_test_pair\t Y_test_skip\t \n",
      "Y_train_cbow\t Y_train_pair\t Y_train_skip\t accuracy\t cbow\t context\t context_id\t context_ids\t data_info\t \n",
      "data_info_cbow\t data_info_pair\t data_info_skip\t data_limit\t datetime\t df\t file_name\t filtered_words_ids\t flatten_one_hot_context_ids\t \n",
      "get_info\t get_info_list\t get_model_list\t get_path_name\t get_target_contexts\t get_unique_file_name\t history_cbow\t history_pair\t history_skip\t \n",
      "id_to_word\t ids\t idx\t index\t inp\t io\t islice\t keras\t layers\t \n",
      "load_model\t loss\t losses\t max_sentence\t max_sentence_lenght\t max_word_count\t min_word_count\t model_cbow\t model_pair\t \n",
      "model_skip\t nltk\t np\t one_hot\t one_hot_context_ids\t one_hot_target_id\t optimizers\t os\t out_m\t \n",
      "out_v\t pad_sequences\t pair\t path\t pd\t physical_devices\t plt\t re\t row\t \n",
      "save_info\t save_model\t section_texts\t sent_tokenize\t sentece_word_ids\t sentence\t sentences\t sequence\t skip\t \n",
      "softmax\t sorted_vocabulary_with_frequency\t stop_words\t stopwords\t sys\t target\t target_id\t text\t texts\t \n",
      "tf\t total_sentences\t total_words\t train_test_split\t training_end\t training_started\t vec\t vocabulary_size\t vocabulary_with_frequency\t \n",
      "weights\t window_size\t word\t word_id\t word_lenght\t word_list\t word_to_id\t word_tokenize\t words\t \n",
      "words_in_current_sentece\t words_without_stop_words\t x\t yaml\t \n"
     ]
    }
   ],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c89e3082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                           Type                    Data/Info\n",
      "--------------------------------------------------------------------\n",
      "Average                            type                    <class '__main__.Average'>\n",
      "Dense                              type                    <class 'keras.layers.core.Dense'>\n",
      "Dropout                            type                    <class 'keras.layers.core.Dropout'>\n",
      "Embedding                          type                    <class 'keras.layers.embeddings.Embedding'>\n",
      "Input                              function                <function Input at 0x16981baf0>\n",
      "Model                              type                    <class 'keras.engine.training.Model'>\n",
      "PlaintextCorpusReader              type                    <class 'nltk.corpus.reade<...>t.PlaintextCorpusReader'>\n",
      "X                                  list                    n=486\n",
      "X_test_cbow                        list                    n=98\n",
      "X_test_pair                        list                    n=450\n",
      "X_test_skip                        list                    n=98\n",
      "X_train_cbow                       list                    n=388\n",
      "X_train_pair                       list                    n=1798\n",
      "X_train_skip                       list                    n=388\n",
      "Y                                  list                    n=2248\n",
      "Y_test_cbow                        list                    n=98\n",
      "Y_test_pair                        list                    n=450\n",
      "Y_test_skip                        list                    n=98\n",
      "Y_train_cbow                       list                    n=388\n",
      "Y_train_pair                       list                    n=1798\n",
      "Y_train_skip                       list                    n=388\n",
      "accuracy                           float                   0.0\n",
      "cbow                               Functional              <keras.engine.functional.<...>al object at 0x2b36c7e20>\n",
      "context                            list                    n=8\n",
      "context_id                         ndarray                 8: 8 elems, type `int32`, 32 bytes\n",
      "context_ids                        ndarray                 486x8: 3888 elems, type `int32`, 15552 bytes\n",
      "data_info                          dict                    n=14\n",
      "data_info_cbow                     dict                    n=18\n",
      "data_info_pair                     dict                    n=17\n",
      "data_info_skip                     dict                    n=18\n",
      "data_limit                         int                     300\n",
      "datetime                           module                  <module 'datetime' from '<...>b/python3.8/datetime.py'>\n",
      "df                                 DataFrame                       ARTICLE_ID       <...>[265139 rows x 4 columns]\n",
      "file_name                          str                     2021-11-19_03-17-44-823361\n",
      "filtered_words_ids                 list                    n=0\n",
      "flatten_one_hot_context_ids        list                    n=486\n",
      "get_info                           function                <function get_info at 0x176f31040>\n",
      "get_info_list                      function                <function get_info_list at 0x176f2bca0>\n",
      "get_model_list                     function                <function get_model_list at 0x176f2b5e0>\n",
      "get_path_name                      function                <function get_path_name at 0x176df6b80>\n",
      "get_target_contexts                function                <function get_target_contexts at 0x176f13280>\n",
      "get_unique_file_name               function                <function get_unique_file_name at 0x176f11040>\n",
      "history_cbow                       History                 <keras.callbacks.History object at 0x2b2e81580>\n",
      "history_pair                       History                 <keras.callbacks.History object at 0x2b29ce490>\n",
      "history_skip                       History                 <keras.callbacks.History object at 0x2b3315f40>\n",
      "id_to_word                         dict                    n=30\n",
      "ids                                list                    n=5\n",
      "idx                                int                     29\n",
      "index                              int                     29\n",
      "inp                                KerasTensor             KerasTensor(type_spec=Ten<...>ated by layer 'input_3'\")\n",
      "io                                 module                  <module 'io' from '/Users<...>Env/lib/python3.8/io.py'>\n",
      "islice                             type                    <class 'itertools.islice'>\n",
      "keras                              LazyLoader              <module 'keras.api._v2.ke<...>i/_v2/keras/__init__.py'>\n",
      "layers                             module                  <module 'keras.api._v2.ke<...>eras/layers/__init__.py'>\n",
      "load_model                         function                <function load_model at 0x176f2b280>\n",
      "loss                               float                   24.81056785583496\n",
      "losses                             module                  <module 'keras.api._v2.ke<...>eras/losses/__init__.py'>\n",
      "max_sentence                       int                     1000\n",
      "max_sentence_lenght                int                     1000\n",
      "max_word_count                     int                     100\n",
      "min_word_count                     int                     5\n",
      "model_cbow                         Functional              <keras.engine.functional.<...>al object at 0x2b2db6520>\n",
      "model_pair                         Functional              <keras.engine.functional.<...>al object at 0x176f58d30>\n",
      "model_skip                         Functional              <keras.engine.functional.<...>al object at 0x2b2b9eaf0>\n",
      "nltk                               module                  <module 'nltk' from '/Use<...>ckages/nltk/__init__.py'>\n",
      "np                                 module                  <module 'numpy' from '/Us<...>kages/numpy/__init__.py'>\n",
      "one_hot                            list                    n=30\n",
      "one_hot_context_ids                list                    n=486\n",
      "one_hot_target_id                  list                    n=486\n",
      "optimizers                         module                  <module 'keras.api._v2.ke<...>/optimizers/__init__.py'>\n",
      "os                                 module                  <module 'os' from '/Users<...>Env/lib/python3.8/os.py'>\n",
      "out_m                              TextIOWrapper           <_io.TextIOWrapper name='<...>ode='w' encoding='utf-8'>\n",
      "out_v                              TextIOWrapper           <_io.TextIOWrapper name='<...>ode='w' encoding='utf-8'>\n",
      "pad_sequences                      function                <function pad_sequences at 0x169877700>\n",
      "pair                               Functional              <keras.engine.functional.<...>al object at 0x2b3288700>\n",
      "path                               str                     save/model_pair/vector_metadata/\n",
      "pd                                 module                  <module 'pandas' from '/U<...>ages/pandas/__init__.py'>\n",
      "physical_devices                   list                    n=1\n",
      "plt                                module                  <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
      "re                                 module                  <module 're' from '/Users<...>Env/lib/python3.8/re.py'>\n",
      "row                                list                    n=30\n",
      "save_info                          function                <function save_info at 0x176f2b940>\n",
      "save_model                         function                <function save_model at 0x176f11c10>\n",
      "section_texts                      Series                  0      \\n\\n\\n\\n\\n\\n'''Ana<...>ength: 300, dtype: object\n",
      "sent_tokenize                      function                <function sent_tokenize at 0x15d4a6ca0>\n",
      "sentece_word_ids                   list                    n=82\n",
      "sentence                           str                     he has been married to fe<...>r steffi graf since 2001.\n",
      "sentences                          list                    n=6661\n",
      "sequence                           module                  <module 'keras.preprocess<...>eprocessing/sequence.py'>\n",
      "skip                               Functional              <keras.engine.functional.<...>al object at 0x2b3387760>\n",
      "softmax                            function                <function softmax at 0x16968e9d0>\n",
      "sorted_vocabulary_with_frequency   list                    n=15960\n",
      "stop_words                         set                     {'only', 'how', \"didn't\",<...>ldn', \"shan't\", 't', 'a'}\n",
      "stopwords                          WordListCorpusReader    <WordListCorpusReader in <...>_data/corpora/stopwords'>\n",
      "sys                                module                  <module 'sys' (built-in)>\n",
      "target                             int                     4\n",
      "target_id                          list                    n=486\n",
      "text                               module                  <module 'keras.preprocess<...>s/preprocessing/text.py'>\n",
      "texts                              str                     \\n\\n\\n'''Andre Kirk Agass<...>Steffi Graf since 2001.\\n\n",
      "tf                                 module                  <module 'tensorflow' from<...>/tensorflow/__init__.py'>\n",
      "total_sentences                    int                     82\n",
      "total_words                        int                     486\n",
      "train_test_split                   function                <function train_test_split at 0x15e1ecd30>\n",
      "training_end                       datetime                2021-11-19 03:17:58.801697\n",
      "training_started                   datetime                2021-11-19 03:17:58.227047\n",
      "vec                                ndarray                 200: 200 elems, type `float32`, 800 bytes\n",
      "vocabulary_size                    int                     30\n",
      "vocabulary_with_frequency          dict                    n=15960\n",
      "weights                            ndarray                 30x200: 6000 elems, type `float32`, 24000 bytes\n",
      "window_size                        int                     4\n",
      "word                               list                    n=30\n",
      "word_id                            int                     30\n",
      "word_lenght                        int                     7\n",
      "word_list                          list                    n=6087\n",
      "word_to_id                         dict                    n=30\n",
      "word_tokenize                      function                <function word_tokenize at 0x15da17af0>\n",
      "words                              list                    n=7\n",
      "words_in_current_sentece           int                     0\n",
      "words_without_stop_words           list                    n=7\n",
      "x                                  KerasTensor             KerasTensor(type_spec=Ten<...>ated by layer 'dense_2'\")\n",
      "yaml                               module                  <module 'yaml' from '/Use<...>ckages/yaml/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c2e07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflowEnv] *",
   "language": "python",
   "name": "conda-env-tensorflowEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

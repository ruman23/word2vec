{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80387ad",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0113bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Dropout\n",
    "from keras.preprocessing import text\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import losses, optimizers\n",
    "from tensorflow.keras.activations import softmax\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fdfc9",
   "metadata": {},
   "source": [
    "### Data Set\n",
    "\n",
    "<br>\n",
    "<b>Total Aritcles:</b> 265139 <br>\n",
    "<b>Total Senteces:</b> 3681645 <br>\n",
    "<b>Total Words:</b> 46842818 <br>\n",
    "<b>Total Unique Words:</b> 634540 <br>\n",
    "\n",
    "<a href=\"https://www.kaggle.com/jkkphys/english-wikipedia-articles-20170820-sqlite/discussion/149578\">Data Source</a>\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d927f",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c09c6622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Head------------------\n",
      "   ARTICLE_ID      TITLE                 SECTION_TITLE  \\\n",
      "0           0  Anarchism                  Introduction   \n",
      "1           0  Anarchism     Etymology and terminology   \n",
      "2           0  Anarchism                       History   \n",
      "3           0  Anarchism  Anarchist schools of thought   \n",
      "4           0  Anarchism   Internal issues and debates   \n",
      "\n",
      "                                        SECTION_TEXT  \n",
      "0  \\n\\n\\n\\n\\n\\n'''Anarchism''' is a political phi...  \n",
      "1  \\n\\nThe term ''anarchism'' is a compound word ...  \n",
      "2  \\n\\n===Origins===\\nWoodcut from a Diggers docu...  \n",
      "3  \\nPortrait of philosopher Pierre-Joseph Proudh...  \n",
      "4  \\nconsistent with anarchist values is a contro...  \n",
      "------------------Tail------------------\n",
      "        ARTICLE_ID              TITLE    SECTION_TITLE  \\\n",
      "265134       30475  Triboluminescence  Further reading   \n",
      "265135       30475  Triboluminescence   External links   \n",
      "265136       30476       Markov chain     Introduction   \n",
      "265137       30476       Markov chain     Introduction   \n",
      "265138       30476       Markov chain          History   \n",
      "\n",
      "                                             SECTION_TEXT  \n",
      "265134                                           * \\n* \\n  \n",
      "265135  \\n* \\n* \\n*  Triboluminescence Discussion on T...  \n",
      "265136  \\nIn probability theory and related fields, a ...  \n",
      "265137  Russian mathematician Andrey Markov.\\nA Markov...  \n",
      "265138  Andrey Markov studied Markov chains in the ear...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('enwiki.csv')\n",
    "print('------------------Head------------------')\n",
    "print(df.head())\n",
    "print('------------------Tail------------------')\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f55548",
   "metadata": {},
   "source": [
    "### Remove unnecessary data from dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795dbd54",
   "metadata": {},
   "source": [
    "# Set constrains for filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db94d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving the information offline\n",
    "data_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc946361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `data_limit` zero (0) for processing all of the data.\n",
    "data_limit = 300\n",
    "vocabulary_size = 50 \n",
    "window_size = 5\n",
    "max_sentence_lenght = 1000\n",
    "max_word_count = 100\n",
    "min_word_count = 5\n",
    "max_sentence = 1000\n",
    "\n",
    "\n",
    "data_info['data_limit']= data_limit\n",
    "data_info['vocabulary_size'] = vocabulary_size\n",
    "data_info['window_size'] = window_size \n",
    "data_info['max_sentence_lenght'] = max_sentence_lenght\n",
    "data_info['max_word_count'] = max_word_count \n",
    "data_info['min_word_count'] = min_word_count\n",
    "data_info['max_sentence'] = max_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a027071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Aritcles -----> 265139\n"
     ]
    }
   ],
   "source": [
    "df.drop(['ARTICLE_ID', 'TITLE', 'SECTION_TITLE'], axis=1)\n",
    "section_texts = df['SECTION_TEXT'].apply(str)\n",
    "print('Total Aritcles ----->', len(section_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f4f6a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total section texts -----> 300\n"
     ]
    }
   ],
   "source": [
    "if data_limit != 0:\n",
    "    section_texts = section_texts[:data_limit]\n",
    "print('Total section texts ----->', len(section_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace7cd0",
   "metadata": {},
   "source": [
    "### Download `nltk` Resources for Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67bc505d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ruman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ruman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7340200",
   "metadata": {},
   "source": [
    "### Convert Aritcles to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c779773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for texts in section_texts:\n",
    "    for sentence in sent_tokenize(texts):\n",
    "        if len(sentence) < max_sentence_lenght:\n",
    "            sentences.append(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a62ec2df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences 6661\n"
     ]
    }
   ],
   "source": [
    "total_sentences = len(sentences)\n",
    "print('Total sentences', total_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d56d9",
   "metadata": {},
   "source": [
    "### Convert Sentences to Words and Create Vocabulary with Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c794f606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "word_list = []\n",
    "vocabulary_with_frequency = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    words_without_stop_words = [word for word in words if word.isalpha() and word not in stop_words and len(word) != 1]\n",
    "    \n",
    "    word_lenght = len(words_without_stop_words)\n",
    "    if word_lenght <= max_word_count and word_lenght >= min_word_count:\n",
    "        word_list.append(words_without_stop_words)\n",
    "        \n",
    "        for word in words_without_stop_words:\n",
    "            if word not in vocabulary_with_frequency.keys():\n",
    "                vocabulary_with_frequency[word] = 1\n",
    "            else:\n",
    "                vocabulary_with_frequency[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015c0bd",
   "metadata": {},
   "source": [
    "### After Filtering Total Words and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c459fec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in corpus 80055\n",
      "Vocabulary size 15960\n"
     ]
    }
   ],
   "source": [
    "total_sentences = len(word_list)\n",
    "\n",
    "total_words = 0\n",
    "for words in word_list:\n",
    "    total_words += len(words)\n",
    "print('Total words in corpus', total_words)\n",
    "print('Vocabulary size', len(vocabulary_with_frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf64ad",
   "metadata": {},
   "source": [
    "### Sort Vocabulary Based on Frequency Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebb1bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocabulary_with_frequency = sorted(vocabulary_with_frequency.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b6cc6",
   "metadata": {},
   "source": [
    "### Remove Less Freuquent Word form Dictionary and Assign Unique Id to Each Word\n",
    "\n",
    "<br> \n",
    "Create two Dictionaries.<br> \n",
    "<b>word_to_id:</b> to get word for a word id. <br>\n",
    "<b>id_to_word:</b> to get the id for word. <br>\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2588df39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'PAD'), ('lincoln', 1), ('also', 2), ('one', 3), ('apollo', 4), ('state', 5), ('first', 6), ('achilles', 7), ('alabama', 8), ('aristotle', 9), ('time', 10), ('autism', 11), ('many', 12), ('new', 13), ('century', 14)]\n",
      "-------------------------------------\n",
      "[('PAD', 0), (1, 'lincoln'), (2, 'also'), (3, 'one'), (4, 'apollo'), (5, 'state'), (6, 'first'), (7, 'achilles'), (8, 'alabama'), (9, 'aristotle'), (10, 'time'), (11, 'autism'), (12, 'many'), (13, 'new'), (14, 'century')]\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {}\n",
    "word_to_id[0] = 'PAD'\n",
    "id_to_word = {}\n",
    "id_to_word['PAD'] = 0\n",
    "word_id = 1\n",
    "\n",
    "for word, _ in sorted_vocabulary_with_frequency:\n",
    "    if word_id < vocabulary_size:\n",
    "        word_to_id[word] = word_id\n",
    "        id_to_word[word_id] = word\n",
    "        word_id += 1\n",
    "        \n",
    "print(list(islice(word_to_id.items(), 15)))\n",
    "print('-------------------------------------')\n",
    "print(list(islice(id_to_word.items(), 15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92629475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dictionary size after removing less frequent words 50\n"
     ]
    }
   ],
   "source": [
    "print('New dictionary size after removing less frequent words', vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3b870",
   "metadata": {},
   "source": [
    "### Convert `word_list` to `word_id_list` for Expressign the Words Vocabualry Id\n",
    "\n",
    "<br> Remove the words which are not present in dictionary</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcb0dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences before filtering 6087\n",
      "Total words before filtering 80055\n"
     ]
    }
   ],
   "source": [
    "data_info['Total sentences before filtering'] = total_sentences\n",
    "data_info['Total words before filtering'] = total_words\n",
    "\n",
    "print('Total sentences before filtering', total_sentences)\n",
    "print('Total words before filtering', total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c02693f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Total sentences after filtering 190\n",
      "Total words after filtering 1141\n"
     ]
    }
   ],
   "source": [
    "sentece_word_ids = []\n",
    "total_sentences = 0\n",
    "total_words = 0\n",
    "\n",
    "for words in word_list:\n",
    "    filtered_words_ids = [word_to_id[word] for word in words if word in word_to_id.keys()]\n",
    "    words_in_current_sentece = len(filtered_words_ids)\n",
    "    \n",
    "    if words_in_current_sentece >= min_word_count:\n",
    "        total_sentences += 1\n",
    "        total_words += words_in_current_sentece\n",
    "        sentece_word_ids.append(filtered_words_ids)\n",
    "        \n",
    "print('--------------------------------------------')\n",
    "print('Total sentences after filtering', total_sentences)\n",
    "print('Total words after filtering', total_words)\n",
    "\n",
    "data_info['Total sentences after filtering'] = total_sentences\n",
    "data_info['Total words after filtering'] = total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211b3a88",
   "metadata": {},
   "source": [
    "### Untill Now \n",
    "<br> \n",
    "<b>sentece_word_ids:</b> Sentence wise word's id. <br>\n",
    "<b>word_to_id:</b> Dictionary for getting the <b>word_id</b> for a <b>word</b>.<br>\n",
    "<b>id_to_word:</b> Dictionary for getting the <b>word</b> for a <b> word_id</b>.<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da05f647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences 190\n",
      "Total words 1141\n",
      "Total unique words in dictionary 50\n"
     ]
    }
   ],
   "source": [
    "print('Total sentences', total_sentences)\n",
    "print('Total words', total_words)\n",
    "vocabulary_size = len(word_to_id)\n",
    "print('Total unique words in dictionary', vocabulary_size)\n",
    "data_info['Total unique words in dictionary'] = vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bedcf3",
   "metadata": {},
   "source": [
    "### Save data for later use\n",
    "It will help to skip the data processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78be2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_data(data, to_file):\n",
    "#     if os.path.exists(to_file+'.yaml'):\n",
    "#         os.remove(to_file+'.yaml')\n",
    "    \n",
    "#     with open(to_file+'.yaml', 'w') as file:\n",
    "#         documents = yaml.dump(data, file, sort_keys=False)\n",
    "    \n",
    "# def load_data(from_file):\n",
    "#     with open(from_file+'.yaml') as file:\n",
    "#         return yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7c7fc",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0f86fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_data(word_to_id, 'word_to_id')\n",
    "# save_data(id_to_word, 'id_to_word')\n",
    "# save_data(sentece_word_ids, 'sentece_word_ids')\n",
    "# # save_data(positive_skip_grams, 'positive_skip_grams')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca271ed1",
   "metadata": {},
   "source": [
    "# ⚠️⚠️⚠️ Reset Everything ⚠️⚠️⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dd4a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8e0a5",
   "metadata": {},
   "source": [
    "# Start again 🏃‍♂️🏃‍♂️🏃‍♂️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ee5f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import sys\n",
    "# import yaml\n",
    "# import os\n",
    "# import io\n",
    "\n",
    "# import nltk\n",
    "# from nltk.corpus import PlaintextCorpusReader\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# from tensorflow.keras import layers\n",
    "# from keras.preprocessing import text\n",
    "\n",
    "# from itertools import islice\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from tensorflow.keras.layers import Embedding, Input, Dense, Dropout\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras import losses, optimizers\n",
    "# from tensorflow.keras.activations import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbfdc047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_data(data, to_file):\n",
    "#     if os.path.exists(to_file+'.yaml'):\n",
    "#         os.remove('id_to_word.yaml')\n",
    "    \n",
    "#     with open(to_file+'.yaml', 'w') as file:\n",
    "#         documents = yaml.dump(data, file, sort_keys=False)\n",
    "    \n",
    "# def load_data(from_file):\n",
    "#     with open(from_file+'.yaml') as file:\n",
    "#         return yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b374e",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1927780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_to_word = load_data('id_to_word')\n",
    "# word_to_id = load_data('word_to_id')\n",
    "# sentece_word_ids = load_data('sentece_word_ids')\n",
    "# # positive_skip_grams = load_data('positive_skip_grams')\n",
    "# vocabulary_size = len(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1005f",
   "metadata": {},
   "source": [
    "#### Example after loadign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec6d81a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(list(islice(id_to_word.items(), 10)))\n",
    "# print('------------------------------------------')\n",
    "# print(list(islice(word_to_id.items(), 10)))\n",
    "# print('------------------------------------------')\n",
    "# print(sentece_word_ids[:5])\n",
    "# print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2e97426",
   "metadata": {},
   "outputs": [],
   "source": [
    "del PlaintextCorpusReader\n",
    "del data_limit\n",
    "del df\n",
    "del filtered_words_ids\n",
    "del max_sentence\n",
    "del max_sentence_lenght\n",
    "del max_word_count\n",
    "del nltk\n",
    "del min_word_count\n",
    "del re \n",
    "del section_texts\n",
    "del sent_tokenize\n",
    "del sentence\n",
    "del sentences\n",
    "del sorted_vocabulary_with_frequency\n",
    "del stop_words\n",
    "del stopwords\n",
    "del text \n",
    "del texts\n",
    "del total_sentences\n",
    "del total_words\n",
    "del vocabulary_with_frequency\n",
    "del word\n",
    "del word_id\n",
    "del word_lenght\n",
    "del word_tokenize               \n",
    "del words                      \n",
    "del words_in_current_sentece   \n",
    "del words_without_stop_words\n",
    "del word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77619d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4165c",
   "metadata": {},
   "source": [
    "### For loading the GPU of Macbook M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adc2d5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929921f9",
   "metadata": {},
   "source": [
    "### Generate 2D `vocabulary_size` List for Getting the `one-hot` Vector for a Word Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d093fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = [[0 for i in range(vocabulary_size)] for j in range(vocabulary_size)]\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    one_hot[index][index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a985fc",
   "metadata": {},
   "source": [
    "### Helper Function Save and Load the Model and Informatino Data\n",
    "\n",
    "In every cases name will be `pari` for pair-model, `skip` for skip-gram model and `cbow` for CBOW model.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abf604e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_name(name):\n",
    "    if name == 'pair':\n",
    "        return 'save/model_pair/'\n",
    "    elif name == 'skip':\n",
    "        return 'save/model_skip/'\n",
    "    elif name == 'cbow':\n",
    "        return 'save/model_cbow/'\n",
    "    \n",
    "def get_unique_file_name():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S-%f\")\n",
    "\n",
    "\n",
    "def save_model(name, model, file_name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    if name == 'cbow':\n",
    "        model.save(path+file_name+'/')\n",
    "    else:\n",
    "         model.save(path+file_name+'.h5')\n",
    "            \n",
    "            \n",
    "def load_model(name, file_name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if name == 'cbow':\n",
    "        print(os.getcwd()+path+file_name)\n",
    "        return keras.models.load_model(\n",
    "            os.getcwd()+'/'+path+'/'+file_name+'/', custom_objects={\"Average\": Average}\n",
    "        )\n",
    "    \n",
    "    return tf.keras.models.load_model(path+file_name+'.h5')\n",
    "\n",
    "\n",
    "def get_model_list(name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        return os.listdir(path)\n",
    "    \n",
    "def save_info(name, file_name, data):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "            \n",
    "    with open(path+file_name+'.yaml', 'w') as file:\n",
    "        documents = yaml.dump(data, file, sort_keys=False)\n",
    "    \n",
    "    \n",
    "def get_info_list(name):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        return os.listdir(path)\n",
    "        \n",
    "\n",
    "def get_info(name, file_name):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "            with open(path+file_name+'.yaml') as file:\n",
    "                return yaml.load(file, Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08e575df",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = get_unique_file_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de79e51",
   "metadata": {},
   "source": [
    "# `Pair-Model`\n",
    "<br> \n",
    "This is the primary version of skip-gram model.<br> As in this model we predict a context word for a target word, that's why I name this model as <b>Pair-Model</b>.\n",
    "\n",
    "For buiding this model at first we need to calculte the positive positive skip-grams for a target and context word. We may also take the negative samples, but in this project I have just used the positive skip-grams.<br>\n",
    "\n",
    "After creating the postivie skip-gram, I have encoded them in one-hot vector. \n",
    "Then used the `target word` as `input` and `context word` as `output`.<br>\n",
    "Also if we want, we may also input the `context word` as `input` and `target word` as `output` to create model as like as `CBOW`. In this proejct I have just created the first one, which one I'm calling is the <b>Pair-Model</b>. \n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df44944",
   "metadata": {},
   "source": [
    "### Copy `data_info` to `data_info_pair` for Saving the Informaiton of Pair-Model to `data_info_pair`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbaad11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_pair = data_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e3e0dd",
   "metadata": {},
   "source": [
    "### Create skip-gram(taget, context) for All of the Words and Convert Them into One-Hot Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9aea7804",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5710, 50)\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "X_target = []\n",
    "Y_context = []\n",
    "\n",
    "for ids in sentece_word_ids:\n",
    "    skip, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "        ids,\n",
    "        vocabulary_size,\n",
    "        window_size=window_size, negative_samples=0,\n",
    "        shuffle=False,\n",
    "        categorical=True)\n",
    "    for target, context in skip:\n",
    "        X_target.append(one_hot[target])\n",
    "        Y_context.append(one_hot[context])\n",
    "print(np.array(X_target).shape)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5252480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_pair['Total pairs of target and context words'] = len(X_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a2d480",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "`20%` data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c54b3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pair, X_test_pair, Y_train_pair, Y_test_pair = train_test_split(X_target, Y_context, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858d9a7d",
   "metadata": {},
   "source": [
    "### Create Model for `Pair-Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d52257e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding_pair (Dense)   (None, 200)               10200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                10050     \n",
      "=================================================================\n",
      "Total params: 20,250\n",
      "Trainable params: 20,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 00:23:15.356085: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-11-20 00:23:15.356199: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(vocabulary_size, ))\n",
    "x = Dense(200, name='w2v_embedding_pair')(inp)\n",
    "x = Dense(vocabulary_size, activation='softmax')(x)\n",
    "\n",
    "model_pair = Model(inputs=inp, outputs=x)\n",
    "model_pair.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016aa52b",
   "metadata": {},
   "source": [
    "### Compile & Fit the Pair-Model\n",
    "\n",
    "Decrease the batch size for saving the RAM :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b298f364",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_pair' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6_/xv_wk6qn5k9g5yztczrn0zlh0000gn/T/ipykernel_6860/3898100108.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtraining_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_pair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mhistory_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_pair' is not defined"
     ]
    }
   ],
   "source": [
    "training_started = datetime.datetime.now()\n",
    "\n",
    "model_pair.compile(loss=losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "history_pair = model_pair.fit(X_train_pair, Y_train_pair, epochs = 3, batch_size=1, verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ee9db39c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_info_pair' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6_/xv_wk6qn5k9g5yztczrn0zlh0000gn/T/ipykernel_6860/1095202486.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtraining_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_info_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Training time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtraining_end\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtraining_started\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_info_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Pair-Model training history'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_pair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_info_pair' is not defined"
     ]
    }
   ],
   "source": [
    "training_end = datetime.datetime.now()\n",
    "data_info_pair['Training time'] =  training_end - training_started\n",
    "data_info_pair['Pair-Model training history'] = history_pair.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7603684",
   "metadata": {},
   "source": [
    "### Save Pair-Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd98a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('pair', model_pair, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f221c91e",
   "metadata": {},
   "source": [
    "### Testing Pair-Model Using Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "415534c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 - 0s - loss: 3.1583 - accuracy: 0.2102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 00:23:48.372086: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_pair.evaluate(X_test_pair, Y_test_pair, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b57bfc34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated loss 3.158266067504883 and accuracy 0.2101576179265976\n"
     ]
    }
   ],
   "source": [
    "data_info_pair['Pair-Model evaluatation loss'] = loss\n",
    "data_info_pair['Pair-Model evaluatation accuracy'] = accuracy\n",
    "print('Evaluated loss',loss, 'and accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc5387",
   "metadata": {},
   "source": [
    "### Save Pair-Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4dd49408",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info('pair',file_name, data_info_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61167fd7",
   "metadata": {},
   "source": [
    "### Save the Vector and Metadata for Pair-Model\n",
    "Projecting the model data, please browse the following link.<br>\n",
    "https://projector.tensorflow.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00715211",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model_pair.get_layer('w2v_embedding_pair').get_weights()[0]\n",
    "\n",
    "path = get_path_name('pair') + 'vector_metadata/'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "out_v = io.open(path+file_name+'vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open(path+file_name+'metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(id_to_word[index] + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0636d03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Pair Model!\n"
     ]
    }
   ],
   "source": [
    "print('End Pair Model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38158f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_target                    \n",
    "del Y_context \n",
    "del X_test_pair          \n",
    "del X_train_pair                              \n",
    "del Y_test_pair            \n",
    "del Y_train_pair          \n",
    "del accuracy              \n",
    "del  context              \n",
    "del data_info_pair        \n",
    "del history_pair         \n",
    "del ids                    \n",
    "del index                  \n",
    "del inp                   \n",
    "del loss                   \n",
    "del out_m                  \n",
    "del out_v                 \n",
    "del path                  \n",
    "del skip                  \n",
    "del softmax               \n",
    "del target                 \n",
    "del training_end          \n",
    "del training_started      \n",
    "del vec                   \n",
    "del weights                \n",
    "del x  \n",
    "del model_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6b430c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4465d45",
   "metadata": {},
   "source": [
    "# Data Processing for the CBOW and SKIP-GRAM Model\n",
    "\n",
    "In CBOW and SKIP-GRAM models, we will be needed the `target word` and their corresponding `context words`. \n",
    "For creating the context word for a target word, we need to consider the total `window_size * 2` words. Among the `window_size * 2`, `window_size` words will be on left side of the target word and `window_size` words fo the right side. If there number of words a target word is less than the `window_size * 2`, then I have use the zero padding. \n",
    "\n",
    "\n",
    "<br> This function creates the target words and their corrosponding context words.<br>\n",
    "return:<br>\n",
    "------<b>target_id:</b> of the target word.<br>\n",
    "------<b>context_ids:</b> of the context words. I have also used the zero padding to make total number of `window_size * 2` target words.<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d003cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_contexts(word_ids, window_size):\n",
    "    target_id = []\n",
    "    context_ids = []\n",
    "    \n",
    "    for ids in word_ids:\n",
    "        for index, word_id in enumerate(ids):\n",
    "            if not word_id:\n",
    "                continue\n",
    "                \n",
    "            window_start = max(0, index - window_size)\n",
    "            window_end = min(len(ids), index + window_size + 1)\n",
    "            \n",
    "            target_id.append(word_id)\n",
    "            context_ids.append([ids[window_index] for window_index in range(window_start, window_end) if window_index != index])\n",
    "                    \n",
    "    return target_id, pad_sequences(context_ids, maxlen = window_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01d545b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id, context_ids =  get_target_contexts(sentece_word_ids, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba2d9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info['Target words'] = len(target_id)\n",
    "data_info['Context words with zero padding'] = len(context_ids) * window_size * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f3453c",
   "metadata": {},
   "source": [
    "### Express `target_id` &  `context_ids` using one-hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26160bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_target_id = [one_hot[id] for id in target_id]\n",
    "one_hot_context_ids = []\n",
    "\n",
    "for context_id in context_ids:\n",
    "    one_hot_context_ids.append([one_hot[id] for id in context_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71949fda",
   "metadata": {},
   "source": [
    "### Take the sum of input vecotor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6963ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for context in one_hot_context_ids:\n",
    "    X.append([sum(x) for x in zip(*context)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a1bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ba7423cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6_/xv_wk6qn5k9g5yztczrn0zlh0000gn/T/ipykernel_6860/4173705141.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mcontext_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mcontext_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtarget_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'context' is not defined"
     ]
    }
   ],
   "source": [
    "del context             \n",
    "del context_id             \n",
    "del context_ids                              \n",
    "del target_id              \n",
    "del one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c0261663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff14e2",
   "metadata": {},
   "source": [
    "# CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a29a8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_cbow =  data_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a09cc4",
   "metadata": {},
   "source": [
    "### Split training testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad66e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cbow, X_test_cbow, Y_train_cbow, Y_test_cbow = train_test_split(X, one_hot_target_id, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a9814",
   "metadata": {},
   "source": [
    "#### Extending keras layer for taking the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d96af11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Average(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Average, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.divide(inputs, window_size*2)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a15ab51b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding_cbow (Dense)   (None, 200)               10200     \n",
      "_________________________________________________________________\n",
      "average (Average)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                10050     \n",
      "=================================================================\n",
      "Total params: 20,250\n",
      "Trainable params: 20,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(vocabulary_size, ))\n",
    "x = Dense(200, name='w2v_embedding_cbow')(inp)\n",
    "x = Average(200)(x)\n",
    "x = Dense(vocabulary_size, activation='softmax')(x)\n",
    "\n",
    "model_cbow = Model(inputs=inp, outputs=x)\n",
    "model_cbow.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6990626",
   "metadata": {},
   "source": [
    "### Fit the CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1b5f942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_started = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a971e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10/19 [==============>...............] - ETA: 0s - loss: 3.9045 - accuracy: 0.0520"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruman/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/backend.py:4846: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n",
      "2021-11-20 00:23:48.867985: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 6ms/step - loss: 3.8936 - accuracy: 0.0976\n",
      "Epoch 2/3\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 3.8423 - accuracy: 0.2281\n",
      "Epoch 3/3\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 3.7912 - accuracy: 0.2851\n"
     ]
    }
   ],
   "source": [
    "model_cbow.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), optimizer='adam', metrics=['accuracy'])\n",
    "history_cbow = model_cbow.fit(X_train_cbow, Y_train_cbow, epochs = 3, batch_size=50, verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ac711ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end = datetime.datetime.now()\n",
    "data_info_cbow['training time'] =  training_end - training_started\n",
    "data_info_cbow['cbow model training history'] = history_cbow.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5237b",
   "metadata": {},
   "source": [
    "### Save CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b33e8ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: save/model_cbow/model/2021-11-20_00-23-15-316636/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 00:23:49.342209: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "save_model('cbow', model_cbow, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439e276",
   "metadata": {},
   "source": [
    "### Testing CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72b76ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 0s - loss: 3.7658 - accuracy: 0.2926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruman/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/backend.py:4846: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n",
      "2021-11-20 00:23:49.607976: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_cbow.evaluate(X_test_cbow, Y_test_cbow, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1529378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated loss 3.765805721282959 and accuracy 0.2925764322280884\n"
     ]
    }
   ],
   "source": [
    "data_info_cbow['cbow model evaluatation loss'] = loss\n",
    "data_info_cbow['cbow model evaluatation accuracy'] = accuracy\n",
    "print('Evaluated loss',loss, 'and accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f931e73",
   "metadata": {},
   "source": [
    "### Save CBOW model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b359bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info('cbow',file_name, data_info_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "217cf891",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_cbow = model_cbow.get_layer('w2v_embedding_cbow').get_weights()[0]\n",
    "\n",
    "path_cbow = get_path_name('cbow') + 'vector_metadata/'\n",
    "\n",
    "\n",
    "if not os.path.exists(path_cbow):\n",
    "        os.makedirs(path_cbow)\n",
    "\n",
    "out_v = io.open(path_cbow+file_name+'vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open(path_cbow+file_name+'metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    vec = weights_cbow[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(id_to_word[index] + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "542756c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of CBOW model\n"
     ]
    }
   ],
   "source": [
    "print('End of CBOW model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c3f497b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X                     \n",
    "del X_test_cbow            \n",
    "del X_train_cbow           \n",
    "del Y_test_cbow            \n",
    "del Y_train_cbow          \n",
    "del accuracy               \n",
    "del data_info_cbow        \n",
    "del history_cbow                     \n",
    "del inp                   \n",
    "del loss                             \n",
    "del pad_sequences          \n",
    "del training_end           \n",
    "del training_started      \n",
    "del x   \n",
    "del model_cbow \n",
    "del out_m\n",
    "del index                                  \n",
    "del out_v                  \n",
    "del path_cbow             \n",
    "del vec                   \n",
    "del weights_cbow           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1c72495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db2008",
   "metadata": {},
   "source": [
    "## SKIP-GRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "285a5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_skip = data_info.copy()\n",
    "del data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca9bb89",
   "metadata": {},
   "source": [
    "### Express the context ids(window_size *2) of a targetd id into a single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ceec85e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_one_hot_context_ids = [sum(one_hot_context_id, []) for one_hot_context_id in one_hot_context_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c59456e",
   "metadata": {},
   "source": [
    "### Split training testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "47a23b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_skip, X_test_skip, Y_train_skip, Y_test_skip = train_test_split(one_hot_target_id, flatten_one_hot_context_ids, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a63d3f",
   "metadata": {},
   "source": [
    "### Create SKIP-GRAMS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a0a285d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding_skip (Dense)   (None, 200)               10200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               100500    \n",
      "=================================================================\n",
      "Total params: 110,700\n",
      "Trainable params: 110,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(vocabulary_size, ))\n",
    "x = Dense(200, name='w2v_embedding_skip')(inp)\n",
    "x = Dense(vocabulary_size * window_size * 2, activation='softmax')(x)\n",
    "model_skip = Model(inputs=inp, outputs=x)\n",
    "model_skip.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464c747",
   "metadata": {},
   "source": [
    "### Fit skip-grams model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b954ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_started = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f199d529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 20/912 [..............................] - ETA: 4s - loss: 33.8878 - accuracy: 0.0000e+00 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 00:23:50.606717: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 5s 5ms/step - loss: 29.7605 - accuracy: 0.0307\n",
      "Epoch 2/5\n",
      "912/912 [==============================] - 5s 5ms/step - loss: 27.2847 - accuracy: 0.0636\n",
      "Epoch 3/5\n",
      "912/912 [==============================] - 5s 5ms/step - loss: 25.8132 - accuracy: 0.0669\n",
      "Epoch 4/5\n",
      "912/912 [==============================] - 5s 5ms/step - loss: 25.2987 - accuracy: 0.0735\n",
      "Epoch 5/5\n",
      "912/912 [==============================] - 5s 5ms/step - loss: 25.5688 - accuracy: 0.0724\n"
     ]
    }
   ],
   "source": [
    "model_skip.compile(loss=losses.categorical_crossentropy, optimizer='rmsprop', metrics=['accuracy'])\n",
    "history_skip = model_skip.fit(X_train_skip, Y_train_skip, epochs = 5, batch_size=1, verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b76fbef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end = datetime.datetime.now()\n",
    "data_info_skip['training time'] =  training_end - training_started\n",
    "data_info_skip['skip-grams model training history'] = history_skip.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddba413c",
   "metadata": {},
   "source": [
    "### Save SKIP_GRAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b192848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('skip', model_skip, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e86d1",
   "metadata": {},
   "source": [
    "### Test SKIP_GRAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2cc896b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 0s - loss: 29.3598 - accuracy: 0.0524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 00:24:14.910679: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_skip.evaluate(X_test_skip, Y_test_skip, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af99f4e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated loss 29.359811782836914 and accuracy 0.052401747554540634\n"
     ]
    }
   ],
   "source": [
    "data_info_skip['skip model evaluatation loss'] = loss\n",
    "data_info_skip['skip model evaluatation accuracy'] = accuracy\n",
    "print('Evaluated loss',loss, 'and accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc45cf",
   "metadata": {},
   "source": [
    "### Save Data info for Skip-Gram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6674ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info('skip',file_name, data_info_skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71849642",
   "metadata": {},
   "source": [
    "### Save Vector and Metadata For SKIP-GRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "af4d0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_skip = model_skip.get_layer('w2v_embedding_skip').get_weights()[0]\n",
    "\n",
    "path_skip = get_path_name('skip') + 'vector_metadata/'\n",
    "\n",
    "\n",
    "if not os.path.exists(path_skip):\n",
    "        os.makedirs(path_skip)\n",
    "\n",
    "out_v = io.open(path_skip+file_name+'vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open(path_skip+file_name+'metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    vec = weights_skip[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(id_to_word[index] + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e15f0ef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Skip-Gram Model\n"
     ]
    }
   ],
   "source": [
    "print('End Skip-Gram Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad6c755",
   "metadata": {},
   "source": [
    "# Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b6a540",
   "metadata": {},
   "source": [
    "### Pair wise input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4335db0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding_pair (Dense)   (None, 200)               10200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                10050     \n",
      "=================================================================\n",
      "Total params: 20,250\n",
      "Trainable params: 20,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pair = load_model('pair', file_name)\n",
    "print(pair.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "344e9d59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_limit': 300,\n",
       " 'vocabulary_size': 50,\n",
       " 'window_size': 5,\n",
       " 'max_sentence_lenght': 1000,\n",
       " 'max_word_count': 100,\n",
       " 'min_word_count': 5,\n",
       " 'max_sentence': 1000,\n",
       " 'Total sentences before filtering': 6087,\n",
       " 'Total words before filtering': 80055,\n",
       " 'Total sentences after filtering': 190,\n",
       " 'Total words after filtering': 1141,\n",
       " 'Total unique words in dictionary': 50,\n",
       " 'Total pairs of target and context words': 5710,\n",
       " 'training time': datetime.timedelta(seconds=32, microseconds=722405),\n",
       " 'pair model training history': {'loss': [3.3004884719848633,\n",
       "   3.0065228939056396,\n",
       "   2.9561688899993896],\n",
       "  'accuracy': [0.19943082332611084, 0.22898423671722412, 0.23095446825027466]},\n",
       " 'Pair-Model evaluatation loss': 3.158266067504883,\n",
       " 'Pair-Model evaluatation accuracy': 0.2101576179265976}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_info('pair', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773814e5",
   "metadata": {},
   "source": [
    "### CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0450cbc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ruman/Documents/project/word2Vec/word2vecsave/model_cbow/model/2021-11-20_00-23-15-316636\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding_cbow (Dense)   (None, 200)               10200     \n",
      "_________________________________________________________________\n",
      "average (Average)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                10050     \n",
      "=================================================================\n",
      "Total params: 20,250\n",
      "Trainable params: 20,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cbow = load_model('cbow', file_name)\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "531ab5cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_limit': 300,\n",
       " 'vocabulary_size': 50,\n",
       " 'window_size': 5,\n",
       " 'max_sentence_lenght': 1000,\n",
       " 'max_word_count': 100,\n",
       " 'min_word_count': 5,\n",
       " 'max_sentence': 1000,\n",
       " 'Total sentences before filtering': 6087,\n",
       " 'Total words before filtering': 80055,\n",
       " 'Total sentences after filtering': 190,\n",
       " 'Total words after filtering': 1141,\n",
       " 'Total unique words in dictionary': 50,\n",
       " 'Target words': 1141,\n",
       " 'Context words with zero padding': 11410,\n",
       " 'training time': datetime.timedelta(microseconds=649136),\n",
       " 'cbow model training history': {'loss': [3.893620491027832,\n",
       "   3.842318058013916,\n",
       "   3.7911999225616455],\n",
       "  'accuracy': [0.0975877195596695, 0.22807016968727112, 0.2850877344608307]},\n",
       " 'cbow model evaluatation loss': 3.765805721282959,\n",
       " 'cbow model evaluatation accuracy': 0.2925764322280884}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_info('cbow', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae11c3",
   "metadata": {},
   "source": [
    "### Skip-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "19878f7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding_skip (Dense)   (None, 200)               10200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               100500    \n",
      "=================================================================\n",
      "Total params: 110,700\n",
      "Trainable params: 110,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "skip = load_model('skip', file_name)\n",
    "print(skip.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0facc9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_limit': 300,\n",
       " 'vocabulary_size': 50,\n",
       " 'window_size': 5,\n",
       " 'max_sentence_lenght': 1000,\n",
       " 'max_word_count': 100,\n",
       " 'min_word_count': 5,\n",
       " 'max_sentence': 1000,\n",
       " 'Total sentences before filtering': 6087,\n",
       " 'Total words before filtering': 80055,\n",
       " 'Total sentences after filtering': 190,\n",
       " 'Total words after filtering': 1141,\n",
       " 'Total unique words in dictionary': 50,\n",
       " 'Target words': 1141,\n",
       " 'Context words with zero padding': 11410,\n",
       " 'training time': datetime.timedelta(seconds=24, microseconds=934082),\n",
       " 'skip-grams model training history': {'loss': [29.76048469543457,\n",
       "   27.284664154052734,\n",
       "   25.813173294067383,\n",
       "   25.298664093017578,\n",
       "   25.568836212158203],\n",
       "  'accuracy': [0.030701754614710808,\n",
       "   0.06359649449586868,\n",
       "   0.06688596308231354,\n",
       "   0.07346491515636444,\n",
       "   0.07236842066049576]},\n",
       " 'skip model evaluatation loss': 29.359811782836914,\n",
       " 'skip model evaluatation accuracy': 0.052401747554540634}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_info('skip', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "988dba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE END\n"
     ]
    }
   ],
   "source": [
    "print('THE END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63804b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflowEnv] *",
   "language": "python",
   "name": "conda-env-tensorflowEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

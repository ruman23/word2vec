{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80387ad",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0113bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import losses, optimizers\n",
    "from tensorflow.keras.activations import softmax\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d927f",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c09c6622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Head------------------\n",
      "   Unnamed: 0     id                                              title  \\\n",
      "0           0  17283  House Republicans Fret About Winning Their Hea...   \n",
      "1           1  17284  Rift Between Officers and Residents as Killing...   \n",
      "2           2  17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
      "3           3  17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
      "4           4  17287  Kim Jong-un Says North Korea Is Preparing to T...   \n",
      "\n",
      "      publication                         author        date    year  month  \\\n",
      "0  New York Times                     Carl Hulse  2016-12-31  2016.0   12.0   \n",
      "1  New York Times  Benjamin Mueller and Al Baker  2017-06-19  2017.0    6.0   \n",
      "2  New York Times                   Margalit Fox  2017-01-06  2017.0    1.0   \n",
      "3  New York Times               William McDonald  2017-04-10  2017.0    4.0   \n",
      "4  New York Times                  Choe Sang-Hun  2017-01-02  2017.0    1.0   \n",
      "\n",
      "   url                                            content  \n",
      "0  NaN  WASHINGTON  —   Congressional Republicans have...  \n",
      "1  NaN  After the bullet shells get counted, the blood...  \n",
      "2  NaN  When Walt Disney’s “Bambi” opened in 1942, cri...  \n",
      "3  NaN  Death may be the great equalizer, but it isn’t...  \n",
      "4  NaN  SEOUL, South Korea  —   North Korea’s leader, ...  \n",
      "------------------Tail------------------\n",
      "       Unnamed: 0     id                                              title  \\\n",
      "49995       53287  73465   Rex Tillerson Says Climate Change Is Real, but …   \n",
      "49996       53288  73466  The Biggest Intelligence Questions Raised by t...   \n",
      "49997       53289  73467  Trump Announces Plan That Does Little to Resol...   \n",
      "49998       53290  73468    Dozens of For-Profit Colleges Could Soon Close    \n",
      "49999       53291  73469                       The Milky Way’s Stolen Stars   \n",
      "\n",
      "      publication          author        date    year  month  url  \\\n",
      "49995    Atlantic  Robinson Meyer  2017-01-11  2017.0    1.0  NaN   \n",
      "49996    Atlantic      Amy Zegart  2017-01-11  2017.0    1.0  NaN   \n",
      "49997    Atlantic   Jeremy Venook  2017-01-11  2017.0    1.0  NaN   \n",
      "49998    Atlantic     Emily DeRuy  2017-01-11  2017.0    1.0  NaN   \n",
      "49999    Atlantic    Marina Koren  2017-01-11  2017.0    1.0  NaN   \n",
      "\n",
      "                                                 content  \n",
      "49995  As chairman and CEO of ExxonMobil, Rex Tillers...  \n",
      "49996  I’ve spent nearly 20 years looking at intellig...  \n",
      "49997    Donald Trump will not be taking necessary st...  \n",
      "49998  Dozens of   colleges could be forced to close ...  \n",
      "49999  The force of gravity can be described using a ...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('articles1.csv')\n",
    "print('------------------Head------------------')\n",
    "print(df.head())\n",
    "print('------------------Tail------------------')\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795dbd54",
   "metadata": {},
   "source": [
    "# Setting Constraints for Filtering the Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc946361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `data_limit` zero (0) for processing all of the data.\n",
    "data_limit = 20000\n",
    "# If the fruquency count of a word is less than the `min_frquency_of_word` than it will be removed from the dictionary\n",
    "min_frquency_of_word = 300 \n",
    "window_size = 7\n",
    "max_sentence_lenght = 1000\n",
    "max_word_count = 100\n",
    "min_word_count = 5\n",
    "max_sentence = 1000\n",
    "\n",
    "data_info = {}\n",
    "data_info['data_limit']= data_limit\n",
    "data_info['min_frquency_of_word'] = min_frquency_of_word\n",
    "data_info['window_size'] = window_size \n",
    "data_info['max_sentence_lenght'] = max_sentence_lenght\n",
    "data_info['max_word_count'] = max_word_count \n",
    "data_info['min_word_count'] = min_word_count\n",
    "data_info['max_sentence'] = max_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a027071",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_texts = df['content'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffda0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f4f6a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total texts-----> 5000\n"
     ]
    }
   ],
   "source": [
    "if data_limit != 0:\n",
    "    section_texts = section_texts[:data_limit]\n",
    "print('Total texts----->', len(section_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01f28371",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace7cd0",
   "metadata": {},
   "source": [
    "### Downloading `nltk` Resources for Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67bc505d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ruman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ruman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7340200",
   "metadata": {},
   "source": [
    "### Converting Texts to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c779773b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for texts in section_texts:\n",
    "    for sentence in sent_tokenize(texts):\n",
    "        if len(sentence) < max_sentence_lenght:\n",
    "            sentences.append(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a62ec2df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences 275244\n"
     ]
    }
   ],
   "source": [
    "total_sentences = len(sentences)\n",
    "print('Total sentences', total_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64a56a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "del texts\n",
    "del section_texts\n",
    "del max_sentence_lenght\n",
    "del sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d56d9",
   "metadata": {},
   "source": [
    "### Filtering out the Sentences, Converting Sentences to Words and Creating Vocabulary with Frequency\n",
    "\n",
    "For filtering out-\n",
    "<ul>\n",
    "  <li>Remove the <b>stop words</b>.</li>\n",
    "  <li>Remove the word if it is not an <b>alpha</b>.</li>\n",
    "  <li>Remove the word if number of character is less than <b>one</b>.</li>\n",
    "  <li>Remove sentences if word count is greater than <b>max_word_count</b> or less than <b>min_word_count</b></li>    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c794f606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "word_list = []\n",
    "vocabulary_with_frequency = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    words_without_stop_words = [word for word in words if word.isalpha() and word not in stop_words and len(word) != 1]\n",
    "    \n",
    "    word_lenght = len(words_without_stop_words)\n",
    "    if word_lenght <= max_word_count and word_lenght >= min_word_count:\n",
    "        word_list.append(words_without_stop_words)\n",
    "        \n",
    "        for word in words_without_stop_words:\n",
    "            if word not in vocabulary_with_frequency.keys():\n",
    "                vocabulary_with_frequency[word] = 1\n",
    "            else:\n",
    "                vocabulary_with_frequency[word] += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be6b93cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76308\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary_with_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4477cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del stop_words\n",
    "del stopwords\n",
    "del sentences\n",
    "del sentence\n",
    "del words\n",
    "del words_without_stop_words\n",
    "del word_lenght\n",
    "del word\n",
    "del max_sentence\n",
    "del max_word_count\n",
    "del nltk\n",
    "del word_tokenize\n",
    "del total_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015c0bd",
   "metadata": {},
   "source": [
    "#### Total Words and Vocabularies After Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c459fec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in corpus 2931103\n",
      "Vocabulary size 76308\n"
     ]
    }
   ],
   "source": [
    "total_sentences = len(word_list)\n",
    "\n",
    "total_words = 0\n",
    "for words in word_list:\n",
    "    total_words += len(words)\n",
    "print('Total words in corpus', total_words)\n",
    "print('Vocabulary size', len(vocabulary_with_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af2cc722",
   "metadata": {},
   "outputs": [],
   "source": [
    "del words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf64ad",
   "metadata": {},
   "source": [
    "### Sorting Vocabulary Based on Frequency Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebb1bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocabulary_with_frequency = sorted(vocabulary_with_frequency.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df438285",
   "metadata": {},
   "source": [
    "### Removing the Word Form the Dictioary if Frequency count is Less Than `min_frquency_of_word`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6da80de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocabulary = {key: frequency for key, frequency in sorted_vocabulary_with_frequency if frequency > min_frquency_of_word}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f61f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sorted_vocabulary_with_frequency\n",
    "del vocabulary_with_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b6cc6",
   "metadata": {},
   "source": [
    "### Removing the Less Freuquent Word form Dictionary and Assigning Unique Id to Each Word\n",
    "\n",
    "Also created two dictionaries-\n",
    "<ul>\n",
    "  <li><b>word_to_id:</b> to get word for a word id.</li>\n",
    "  <li><b>id_to_word:</b> to get the id for word.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2588df39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'PAD'), ('said', 1), ('trump', 2), ('would', 3), ('one', 4), ('new', 5), ('people', 6), ('like', 7), ('also', 8), ('president', 9), ('could', 10), ('years', 11), ('two', 12), ('time', 13), ('states', 14)]\n",
      "-------------------------------------\n",
      "[('PAD', 0), (1, 'said'), (2, 'trump'), (3, 'would'), (4, 'one'), (5, 'new'), (6, 'people'), (7, 'like'), (8, 'also'), (9, 'president'), (10, 'could'), (11, 'years'), (12, 'two'), (13, 'time'), (14, 'states')]\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {}\n",
    "word_to_id[0] = 'PAD'\n",
    "id_to_word = {}\n",
    "id_to_word['PAD'] = 0\n",
    "word_id = 1\n",
    "\n",
    "for word in sorted_vocabulary:\n",
    "    word_to_id[word] = word_id\n",
    "    id_to_word[word_id] = word\n",
    "    word_id += 1\n",
    "print(list(islice(word_to_id.items(), 15)))\n",
    "print('-------------------------------------')\n",
    "print(list(islice(id_to_word.items(), 15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c6584aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sorted_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3b870",
   "metadata": {},
   "source": [
    "### Converting `word_list` to `word_id_list` for Expressign the Words by Unique Dincatioanry ID  \n",
    "In this step I have also--\n",
    "<ul>\n",
    "  <li>Removed a word if that is not in the dictionary</li>\n",
    "  <li>Removed a sentece if word count is less than the <b>min_word_count</b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcb0dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences before filtering 226168\n",
      "Total words before filtering 2931103\n"
     ]
    }
   ],
   "source": [
    "data_info['Total sentences before filtering'] = total_sentences\n",
    "data_info['Total words before filtering'] = total_words\n",
    "\n",
    "print('Total sentences before filtering', total_sentences)\n",
    "print('Total words before filtering', total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c02693f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Total sentences after filtering 175421\n",
      "Total words after filtering 1639095\n"
     ]
    }
   ],
   "source": [
    "sentece_word_ids = []\n",
    "total_sentences = 0\n",
    "total_words = 0\n",
    "\n",
    "for words in word_list:\n",
    "    filtered_words_ids = [word_to_id[word] for word in words if word in word_to_id.keys()]\n",
    "    words_in_current_sentece = len(filtered_words_ids)\n",
    "    \n",
    "    if words_in_current_sentece >= min_word_count:\n",
    "        total_sentences += 1\n",
    "        total_words += words_in_current_sentece\n",
    "        sentece_word_ids.append(filtered_words_ids)\n",
    "        \n",
    "print('--------------------------------------------')\n",
    "print('Total sentences after filtering', total_sentences)\n",
    "print('Total words after filtering', total_words)\n",
    "\n",
    "data_info['Total sentences after filtering'] = total_sentences\n",
    "data_info['Total words after filtering'] = total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35015a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "del words\n",
    "del word_list\n",
    "del min_word_count                     \n",
    "del word_id  \n",
    "del filtered_words_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211b3a88",
   "metadata": {},
   "source": [
    "## Untill Now \n",
    "<br> \n",
    "<b>sentece_word_ids:</b> Sentence wise word's id. <br>\n",
    "<b>word_to_id:</b> Dictionary for getting the <b>word_id</b> for a <b>word</b>.<br>\n",
    "<b>id_to_word:</b> Dictionary for getting the <b>word</b> for a <b> word_id</b>.<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da05f647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences 175421\n",
      "Total words 1639095\n",
      "Total unique words in dictionary 1851\n"
     ]
    }
   ],
   "source": [
    "print('Total sentences', total_sentences)\n",
    "print('Total words', total_words)\n",
    "vocabulary_size = len(word_to_id)\n",
    "print('Total unique words in dictionary', vocabulary_size)\n",
    "data_info['Total unique words in dictionary'] = vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4165c",
   "metadata": {},
   "source": [
    "### Loading the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adc2d5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a985fc",
   "metadata": {},
   "source": [
    "### Helper Function for Saving and Loading the Model, Informatino\n",
    "\n",
    "In every cases name will be `skip` for skip-gram model and `cbow` for CBOW model.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abf604e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_name(name):\n",
    "    if name == 'pair':\n",
    "        return 'save/model_pair/'\n",
    "    elif name == 'skip':\n",
    "        return 'save/model_skip/'\n",
    "    elif name == 'cbow':\n",
    "        return 'save/model_cbow/'\n",
    "    \n",
    "def get_unique_file_name():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S-%f\")\n",
    "\n",
    "\n",
    "def save_model(name, model, file_name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    if name == 'cbow':\n",
    "        model.save(path+file_name+'/')\n",
    "    else:\n",
    "         model.save(path+file_name+'.h5')\n",
    "            \n",
    "            \n",
    "def load_model(name, file_name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if name == 'cbow':\n",
    "        return keras.models.load_model(\n",
    "            os.getcwd()+'/'+path+'/'+file_name+'/', custom_objects={\"Average\": Average, \"custom_loss_function\": custom_loss_function}\n",
    "        )\n",
    "    \n",
    "    return tf.keras.models.load_model(path+file_name+'.h5', custom_objects={\"custom_loss_function\": custom_loss_function})\n",
    "\n",
    "\n",
    "def get_model_list(name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        return os.listdir(path)\n",
    "    \n",
    "def save_info(name, file_name, data):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "            \n",
    "    with open(path+file_name+'.yaml', 'w') as file:\n",
    "        documents = yaml.dump(data, file, sort_keys=False)\n",
    "    \n",
    "    \n",
    "def get_info_list(name):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        return os.listdir(path)\n",
    "        \n",
    "\n",
    "def get_info(name, file_name):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "            with open(path+file_name+'.yaml') as file:\n",
    "                return yaml.load(file, Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08e575df",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = get_unique_file_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929921f9",
   "metadata": {},
   "source": [
    "### Generate 2D `vocabulary_size` List for Getting the `one-hot` Vector for a Word Id\n",
    "\n",
    "`one_hot` will work as a `lookup table` to get one-hot encoded vector for a `word_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7eb90fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = np.zeros((vocabulary_size,vocabulary_size))\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    one_hot[index][index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4465d45",
   "metadata": {},
   "source": [
    "# Processing Data for the CBOW and SKIP-GRAM Model\n",
    "\n",
    "In `CBOW` and `SKIP-GRAM` models, will be needed the one-hot encoded `target word` and their corresponding one-hot encoded `context words`. And the size of the one-hot encoded vector will be `vocabulary_size`. \n",
    "\n",
    "One-hot encoded `target_id` and `context_ids` creating process. \n",
    "\n",
    "---<b>target_id: </b>Created a zeros vector of `vocabulary_size`. Then simply assigned the value `one(1)` to an index of the vector, if index number is equal to the `word_id`<br>\n",
    "---<b>context_ids:</b> Created total `window_size * 2`(maximum) `context_ids` for a `word_id`. Then I have created one-hot vector for `context_ids`, using the similar way I have created one-hot vector for a `target_id`. Additionaly, I have also sum up all of the one-hot encoded `context_ids` vectors to a single one-hot encoded vector of size `vocabulary_size`. So, there is one vector of size `vocabulary_size`, for containing all of the `context_ids` of a `target_id`. Because in `CBOW` model, at input layer and in `skip-gram` model at output layer, I have to sum up the one-hot context vectors. Here I have precalcualted it here.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d003cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_contexts(word_ids, window_size):\n",
    "    target_id = []\n",
    "    context_ids = []\n",
    "    \n",
    "    for ids in word_ids:\n",
    "        for index, word_id in enumerate(ids):\n",
    "            if not word_id:\n",
    "                continue\n",
    "                \n",
    "            window_start = max(0, index - window_size)\n",
    "            window_end = min(len(ids), index + window_size + 1)\n",
    "            \n",
    "            target_id.append(one_hot[word_id])\n",
    "            \n",
    "            zero_context_ids = np.zeros(vocabulary_size)\n",
    "            for window_index in range(window_start, window_end):\n",
    "                if window_index != index:\n",
    "                    zero_context_ids[ids[window_index]] += 1\n",
    "            context_ids.append(zero_context_ids)\n",
    "            del zero_context_ids\n",
    "            \n",
    "    return target_id, context_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0901db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "del index                      \n",
    "del min_frquency_of_word       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01d545b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id, context_ids =  get_target_contexts(sentece_word_ids, window_size)\n",
    "data_info['Total target words'] = len(target_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910532d8",
   "metadata": {},
   "source": [
    "### Splitting the Training and Testing Data. Taking 20% Data for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad66e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_target_list, X_test_target_list, Y_train_contexts_list, Y_test_contexts_list = train_test_split(target_id, context_ids, test_size=0.2, random_state=42)\n",
    "del target_id\n",
    "del context_ids\n",
    "Train_target = np.array(X_train_target_list, dtype=np.float16)\n",
    "del X_train_target_list\n",
    "Test_target = np.array(X_test_target_list, dtype=np.float16)\n",
    "del X_test_target_list\n",
    "Train_contexts = np.array(Y_train_contexts_list, dtype=np.float16)\n",
    "del Y_train_contexts_list\n",
    "Test_contexts = np.array(Y_test_contexts_list, dtype=np.float16)\n",
    "del Y_test_contexts_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff14e2",
   "metadata": {},
   "source": [
    "# CBOW Model\n",
    "\n",
    "In `CBOW` model our input will be the vector of `context_ids` to predict the `target_id`. So, `Train_contexts` will be our input and the target will be `Train_target`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a29a8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_cbow =  data_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a9814",
   "metadata": {},
   "source": [
    "### Extending Keras Layer for Taking the Average of Embeddings Layer\n",
    "\n",
    "I have alredys sum up of the input vectors. So, in this layer I've just taking the average after multiplying the the input vector with weight matrix(input layer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d96af11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Average(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Average, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.divide(inputs, window_size*2)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91cd754",
   "metadata": {},
   "source": [
    "### Building the model for CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a15ab51b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1851)]            0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding_cbow (Dense)   (None, 400)               740800    \n",
      "_________________________________________________________________\n",
      "average (Average)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1851)              742251    \n",
      "=================================================================\n",
      "Total params: 1,483,051\n",
      "Trainable params: 1,483,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 23:25:41.360831: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-11-24 23:25:41.360973: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(vocabulary_size, ))\n",
    "x = Dense(400, name='w2v_embedding_cbow')(inp)\n",
    "x = Average(200)(x)\n",
    "x = Dense(vocabulary_size, activation='softmax')(x)\n",
    "\n",
    "model_cbow = Model(inputs=inp, outputs=x)\n",
    "model_cbow.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6990626",
   "metadata": {},
   "source": [
    "### Fitting the CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a971e6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6_/xv_wk6qn5k9g5yztczrn0zlh0000gn/T/ipykernel_1390/796103233.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtraining_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_cbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_loss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhistory_cbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_contexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1132\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1135\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1139\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m                **kwargs):\n\u001b[1;32m    229\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    232\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0m_is_scipy_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m   \"\"\"\n\u001b[0;32m-> 1430\u001b[0;31m   return convert_to_tensor_v2(\n\u001b[0m\u001b[1;32m   1431\u001b[0m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1434\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   \u001b[0;34m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m   return convert_to_tensor(\n\u001b[0m\u001b[1;32m   1437\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m   \"\"\"\n\u001b[0;32m--> 271\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    272\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "def custom_loss_function(y_true, y_pred):\n",
    "    return tf.reduce_mean(-tf.math.reduce_sum(y_true * tf.math.log(y_pred), axis=[1]))\n",
    "\n",
    "\n",
    "training_started = datetime.datetime.now()\n",
    "model_cbow.compile(loss=custom_loss_function, optimizer=tf.optimizers.SGD(learning_rate=0.9), metrics=['accuracy'])\n",
    "history_cbow = model_cbow.fit(Train_contexts, Train_target, epochs = 100, batch_size=100, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac711ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end = datetime.datetime.now()\n",
    "data_info_cbow['training time'] =  training_end - training_started\n",
    "data_info_cbow['cbow model training history'] = history_cbow.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5237b",
   "metadata": {},
   "source": [
    "### Saving CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e8ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_model('cbow', model_cbow, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439e276",
   "metadata": {},
   "source": [
    "### Testing CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b76ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_cbow.evaluate(Test_contexts, Test_target, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_cbow['cbow model evaluatation loss'] = loss\n",
    "data_info_cbow['cbow model evaluatation accuracy'] = accuracy\n",
    "print('Evaluated loss',loss, 'and accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f931e73",
   "metadata": {},
   "source": [
    "### Saving Information for CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b359bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info('cbow',file_name, data_info_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cf891",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_cbow = model_cbow.get_layer('w2v_embedding_cbow').get_weights()[0]\n",
    "\n",
    "path_cbow = get_path_name('cbow') + 'vector_metadata/'\n",
    "\n",
    "\n",
    "if not os.path.exists(path_cbow):\n",
    "        os.makedirs(path_cbow)\n",
    "\n",
    "out_v = io.open(path_cbow+file_name+'vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open(path_cbow+file_name+'metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    vec = weights_cbow[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(id_to_word[index] + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542756c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('End of CBOW model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del accuracy                   \n",
    "del custom_loss_function       \n",
    "del data_info_cbow            \n",
    "del history_cbow              \n",
    "del index                      \n",
    "del inp                       \n",
    "del model_cbow                 \n",
    "del out_m                      \n",
    "del out_v                     \n",
    "del path_cbow                  \n",
    "del train_test_split          \n",
    "del training_end              \n",
    "del training_started           \n",
    "del vec                        \n",
    "del weights_cbow               \n",
    "del words_in_current_sentece   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db2008",
   "metadata": {},
   "source": [
    "## SKIP-GRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_skip = data_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a63d3f",
   "metadata": {},
   "source": [
    "### Creating SKIP-GRAMS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a285d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(vocabulary_size, ))\n",
    "x = Dense(500, name='w2v_embedding_skip', bias_initializer=tf.initializers.RandomNormal(stddev=1.0))(inp)\n",
    "x = Dense(vocabulary_size, bias_initializer=tf.initializers.RandomNormal(stddev=1.0), activation='softmax')(x)\n",
    "model_skip = Model(inputs=inp, outputs=x)\n",
    "model_skip.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464c747",
   "metadata": {},
   "source": [
    "### Fitting Skip-Grams Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_started = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_function(y_true, y_pred):\n",
    "    return tf.reduce_mean(-tf.math.reduce_sum(y_true * tf.math.log(y_pred), axis=[1]))\n",
    "\n",
    "\n",
    "model_skip.compile(loss=custom_loss_function, optimizer=tf.optimizers.SGD(learning_rate=0.01), metrics=['accuracy'])\n",
    "history_skip = model_skip.fit(Train_target, Train_contexts, epochs = 40, batch_size=100, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a04383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_function(y_true, y_pred):\n",
    "    return tf.reduce_mean(-tf.math.reduce_sum(y_true * tf.math.log(y_pred), axis=[1]))\n",
    "\n",
    "\n",
    "model_skip.compile(loss=custom_loss_function, optimizer=tf.optimizers.SGD(learning_rate=0.01), metrics=['accuracy'])\n",
    "history_skip = model_skip.fit(Train_target, Train_contexts, epochs = 5, batch_size=2, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43454535",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76fbef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end = datetime.datetime.now()\n",
    "data_info_skip['training time'] =  training_end - training_started\n",
    "data_info_skip['skip-grams model training history'] = history_skip.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddba413c",
   "metadata": {},
   "source": [
    "### Saving SKIP_GRAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b192848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('skip', model_skip, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e86d1",
   "metadata": {},
   "source": [
    "### Testing SKIP_GRAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc896b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_skip.evaluate(Test_target, Test_contexts, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99f4e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_info_skip['skip model evaluatation loss'] = loss\n",
    "data_info_skip['skip model evaluatation accuracy'] = accuracy\n",
    "print('Evaluated loss',loss, 'and accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc45cf",
   "metadata": {},
   "source": [
    "### Saving Information for Skip-Gram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6674ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info('skip',file_name, data_info_skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71849642",
   "metadata": {},
   "source": [
    "### Saving Vector and Metadata For SKIP-GRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_skip = model_skip.get_layer('w2v_embedding_skip').get_weights()[0]\n",
    "\n",
    "path_skip = get_path_name('skip') + 'vector_metadata/'\n",
    "\n",
    "\n",
    "if not os.path.exists(path_skip):\n",
    "        os.makedirs(path_skip)\n",
    "\n",
    "out_v = io.open(path_skip+file_name+'vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open(path_skip+file_name+'metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    vec = weights_skip[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(id_to_word[index] + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f0ef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('End Skip-Gram Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad6c755",
   "metadata": {},
   "source": [
    "# Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773814e5",
   "metadata": {},
   "source": [
    "### CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450cbc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cbow = load_model('cbow', file_name)\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ab5cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_info('cbow', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae11c3",
   "metadata": {},
   "source": [
    "### Skip-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19878f7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "skip = load_model('skip', file_name)\n",
    "print(skip.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0facc9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_info('skip', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988dba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('THE END')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflowEnv] *",
   "language": "python",
   "name": "conda-env-tensorflowEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

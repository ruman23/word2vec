{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80387ad",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0113bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Dropout\n",
    "from keras.preprocessing import text\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import losses, optimizers\n",
    "from tensorflow.keras.activations import softmax\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fdfc9",
   "metadata": {},
   "source": [
    "### Data Set\n",
    "\n",
    "<br>\n",
    "<b>Total Aritcles:</b> 265139 <br>\n",
    "<b>Total Senteces:</b> 3681645 <br>\n",
    "<b>Total Words:</b> 46842818 <br>\n",
    "<b>Total Unique Words:</b> 634540 <br>\n",
    "\n",
    "<a href=\"https://www.kaggle.com/jkkphys/english-wikipedia-articles-20170820-sqlite/discussion/149578\">Data Source</a>\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d927f",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c09c6622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Head------------------\n",
      "   Class Index                                              Title  \\\n",
      "0            3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
      "1            3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
      "2            3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
      "3            3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
      "4            3  Oil prices soar to all-time record, posing new...   \n",
      "\n",
      "                                         Description  \n",
      "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
      "1  Reuters - Private investment firm Carlyle Grou...  \n",
      "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
      "3  Reuters - Authorities have halted oil export\\f...  \n",
      "4  AFP - Tearaway world oil prices, toppling reco...  \n",
      "------------------Tail------------------\n",
      "        Class Index                                              Title  \\\n",
      "119995            1  Pakistan's Musharraf Says Won't Quit as Army C...   \n",
      "119996            2                  Renteria signing a top-shelf deal   \n",
      "119997            2                    Saban not going to Dolphins yet   \n",
      "119998            2                                  Today's NFL games   \n",
      "119999            2                       Nets get Carter from Raptors   \n",
      "\n",
      "                                              Description  \n",
      "119995   KARACHI (Reuters) - Pakistani President Perve...  \n",
      "119996  Red Sox general manager Theo Epstein acknowled...  \n",
      "119997  The Miami Dolphins will put their courtship of...  \n",
      "119998  PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: ...  \n",
      "119999  INDIANAPOLIS -- All-Star Vince Carter was trad...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train_ag.csv')\n",
    "print('------------------Head------------------')\n",
    "print(df.head())\n",
    "print('------------------Tail------------------')\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f55548",
   "metadata": {},
   "source": [
    "### Remove unnecessary data from dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795dbd54",
   "metadata": {},
   "source": [
    "# Set constrains for filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db94d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving the information offline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc946361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `data_limit` zero (0) for processing all of the data.\n",
    "data_limit = 0\n",
    "vocabulary_size = 400 \n",
    "window_size = 5\n",
    "max_sentence_lenght = 1000\n",
    "max_word_count = 100\n",
    "min_word_count = 5\n",
    "max_sentence = 1000\n",
    "\n",
    "data_info = {}\n",
    "data_info['data_limit']= data_limit\n",
    "data_info['vocabulary_size'] = vocabulary_size\n",
    "data_info['window_size'] = window_size \n",
    "data_info['max_sentence_lenght'] = max_sentence_lenght\n",
    "data_info['max_word_count'] = max_word_count \n",
    "data_info['min_word_count'] = min_word_count\n",
    "data_info['max_sentence'] = max_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "874170f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_texts = df['Description'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a027071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['ARTICLE_ID', 'TITLE', 'SECTION_TITLE'], axis=1)\n",
    "# section_texts = df['SECTION_TEXT'].apply(str)\n",
    "# print('Total Aritcles ----->', len(section_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6f4f6a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total section texts -----> 120000\n"
     ]
    }
   ],
   "source": [
    "if data_limit != 0:\n",
    "    section_texts = section_texts[:data_limit]\n",
    "print('Total section texts ----->', len(section_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace7cd0",
   "metadata": {},
   "source": [
    "### Download `nltk` Resources for Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67bc505d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ruman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ruman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5a4cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = open(\"data.txt\").readlines()\n",
    "\n",
    "# sentences = []\n",
    "# for texts in data:\n",
    "#     for sentence in sent_tokenize(texts):\n",
    "#         if len(sentence) < max_sentence_lenght:\n",
    "#             sentences.append(sentence.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7340200",
   "metadata": {},
   "source": [
    "### Convert Aritcles to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c779773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for texts in section_texts:\n",
    "    for sentence in sent_tokenize(texts):\n",
    "        if len(sentence) < max_sentence_lenght:\n",
    "            sentences.append(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a62ec2df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences 154346\n"
     ]
    }
   ],
   "source": [
    "total_sentences = len(sentences)\n",
    "print('Total sentences', total_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d56d9",
   "metadata": {},
   "source": [
    "### Convert Sentences to Words and Create Vocabulary with Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c794f606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "word_list = []\n",
    "vocabulary_with_frequency = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    words_without_stop_words = [word for word in words if word.isalpha() and word not in stop_words and len(word) != 1]\n",
    "    \n",
    "    word_lenght = len(words_without_stop_words)\n",
    "    if word_lenght <= max_word_count and word_lenght >= min_word_count:\n",
    "        word_list.append(words_without_stop_words)\n",
    "        \n",
    "        for word in words_without_stop_words:\n",
    "            if word not in vocabulary_with_frequency.keys():\n",
    "                vocabulary_with_frequency[word] = 1\n",
    "            else:\n",
    "                vocabulary_with_frequency[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015c0bd",
   "metadata": {},
   "source": [
    "### After Filtering Total Words and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c459fec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in corpus 2212850\n",
      "Vocabulary size 55271\n"
     ]
    }
   ],
   "source": [
    "total_sentences = len(word_list)\n",
    "\n",
    "total_words = 0\n",
    "for words in word_list:\n",
    "    total_words += len(words)\n",
    "print('Total words in corpus', total_words)\n",
    "print('Vocabulary size', len(vocabulary_with_frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf64ad",
   "metadata": {},
   "source": [
    "### Sort Vocabulary Based on Frequency Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebb1bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocabulary_with_frequency = sorted(vocabulary_with_frequency.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b6cc6",
   "metadata": {},
   "source": [
    "### Remove Less Freuquent Word form Dictionary and Assign Unique Id to Each Word\n",
    "\n",
    "<br> \n",
    "Create two Dictionaries.<br> \n",
    "<b>word_to_id:</b> to get word for a word id. <br>\n",
    "<b>id_to_word:</b> to get the id for word. <br>\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2588df39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'PAD'), ('said', 1), ('new', 2), ('reuters', 3), ('quot', 4), ('us', 5), ('ap', 6), ('two', 7), ('gt', 8), ('lt', 9), ('first', 10), ('monday', 11), ('wednesday', 12), ('tuesday', 13), ('company', 14)]\n",
      "-------------------------------------\n",
      "[('PAD', 0), (1, 'said'), (2, 'new'), (3, 'reuters'), (4, 'quot'), (5, 'us'), (6, 'ap'), (7, 'two'), (8, 'gt'), (9, 'lt'), (10, 'first'), (11, 'monday'), (12, 'wednesday'), (13, 'tuesday'), (14, 'company')]\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {}\n",
    "word_to_id[0] = 'PAD'\n",
    "id_to_word = {}\n",
    "id_to_word['PAD'] = 0\n",
    "word_id = 1\n",
    "\n",
    "for word, _ in sorted_vocabulary_with_frequency:\n",
    "    if word_id < vocabulary_size:\n",
    "        word_to_id[word] = word_id\n",
    "        id_to_word[word_id] = word\n",
    "        word_id += 1\n",
    "        \n",
    "print(list(islice(word_to_id.items(), 15)))\n",
    "print('-------------------------------------')\n",
    "print(list(islice(id_to_word.items(), 15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92629475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dictionary size after removing less frequent words 400\n"
     ]
    }
   ],
   "source": [
    "print('New dictionary size after removing less frequent words', vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3b870",
   "metadata": {},
   "source": [
    "### Convert `word_list` to `word_id_list` for Expressign the Words Vocabualry Id\n",
    "\n",
    "<br> Remove the words which are not present in dictionary</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcb0dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences before filtering 143642\n",
      "Total words before filtering 2212850\n"
     ]
    }
   ],
   "source": [
    "data_info['Total sentences before filtering'] = total_sentences\n",
    "data_info['Total words before filtering'] = total_words\n",
    "\n",
    "print('Total sentences before filtering', total_sentences)\n",
    "print('Total words before filtering', total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c02693f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Total sentences after filtering 86235\n",
      "Total words after filtering 689781\n"
     ]
    }
   ],
   "source": [
    "sentece_word_ids = []\n",
    "total_sentences = 0\n",
    "total_words = 0\n",
    "\n",
    "for words in word_list:\n",
    "    filtered_words_ids = [word_to_id[word] for word in words if word in word_to_id.keys()]\n",
    "    words_in_current_sentece = len(filtered_words_ids)\n",
    "    \n",
    "    if words_in_current_sentece >= min_word_count:\n",
    "        total_sentences += 1\n",
    "        total_words += words_in_current_sentece\n",
    "        sentece_word_ids.append(filtered_words_ids)\n",
    "        \n",
    "print('--------------------------------------------')\n",
    "print('Total sentences after filtering', total_sentences)\n",
    "print('Total words after filtering', total_words)\n",
    "\n",
    "data_info['Total sentences after filtering'] = total_sentences\n",
    "data_info['Total words after filtering'] = total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211b3a88",
   "metadata": {},
   "source": [
    "### Untill Now \n",
    "<br> \n",
    "<b>sentece_word_ids:</b> Sentence wise word's id. <br>\n",
    "<b>word_to_id:</b> Dictionary for getting the <b>word_id</b> for a <b>word</b>.<br>\n",
    "<b>id_to_word:</b> Dictionary for getting the <b>word</b> for a <b> word_id</b>.<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da05f647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences 86235\n",
      "Total words 689781\n",
      "Total unique words in dictionary 400\n"
     ]
    }
   ],
   "source": [
    "print('Total sentences', total_sentences)\n",
    "print('Total words', total_words)\n",
    "vocabulary_size = len(word_to_id)\n",
    "print('Total unique words in dictionary', vocabulary_size)\n",
    "data_info['Total unique words in dictionary'] = vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4165c",
   "metadata": {},
   "source": [
    "### For loading the GPU of Macbook M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adc2d5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a985fc",
   "metadata": {},
   "source": [
    "### Helper Function Save and Load the Model and Informatino Data\n",
    "\n",
    "In every cases name will be `pari` for pair-model, `skip` for skip-gram model and `cbow` for CBOW model.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abf604e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_name(name):\n",
    "    if name == 'pair':\n",
    "        return 'save/model_pair/'\n",
    "    elif name == 'skip':\n",
    "        return 'save/model_skip/'\n",
    "    elif name == 'cbow':\n",
    "        return 'save/model_cbow/'\n",
    "    \n",
    "def get_unique_file_name():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S-%f\")\n",
    "\n",
    "\n",
    "def save_model(name, model, file_name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    if name == 'cbow':\n",
    "        model.save(path+file_name+'/')\n",
    "    else:\n",
    "         model.save(path+file_name+'.h5')\n",
    "            \n",
    "            \n",
    "def load_model(name, file_name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if name == 'cbow':\n",
    "        print(os.getcwd()+path+file_name)\n",
    "        return keras.models.load_model(\n",
    "            os.getcwd()+'/'+path+'/'+file_name+'/', custom_objects={\"Average\": Average}\n",
    "        )\n",
    "    \n",
    "    return tf.keras.models.load_model(path+file_name+'.h5')\n",
    "\n",
    "\n",
    "def get_model_list(name):\n",
    "    path = get_path_name(name)+'model/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        return os.listdir(path)\n",
    "    \n",
    "def save_info(name, file_name, data):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "            \n",
    "    with open(path+file_name+'.yaml', 'w') as file:\n",
    "        documents = yaml.dump(data, file, sort_keys=False)\n",
    "    \n",
    "    \n",
    "def get_info_list(name):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        return os.listdir(path)\n",
    "        \n",
    "\n",
    "def get_info(name, file_name):\n",
    "    path = get_path_name(name)+'data_info/'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "            with open(path+file_name+'.yaml') as file:\n",
    "                return yaml.load(file, Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08e575df",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = get_unique_file_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929921f9",
   "metadata": {},
   "source": [
    "### Generate 2D `vocabulary_size` List for Getting the `one-hot` Vector for a Word Id\n",
    "\n",
    "Purpose\n",
    "<br>\n",
    "If we want to get/create a one-hot vector for a word_id, than just we have to call the corrosponding row or column of from the one_hot matrix.\n",
    "<br>\n",
    "<br>\n",
    "All the elements of the first row or column of the one_hot is zeror. So, If we need a zerro containing(All elemtns are zeror) one-hot vecotr, than just need to take the first row or the first column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7eb90fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = np.zeros((vocabulary_size,vocabulary_size))\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    one_hot[index][index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4465d45",
   "metadata": {},
   "source": [
    "# Data Processing for the CBOW and SKIP-GRAM Model\n",
    "\n",
    "In `CBOW` and `SKIP-GRAM` models, we will be needed the `target word` and their corresponding `context words`. \n",
    "For creating the context word for a target word, we need to consider total `window_size * 2` words. Among the `window_size * 2` words, `window_size` words will be on left side of the target word and `window_size` words will be on the right side. If the number of context words for target word is less than the `window_size * 2`, then I have use the zero padding. \n",
    "\n",
    "\n",
    "<br> This function creates the target words and their corrosponding context words.<br>\n",
    "return:<br>\n",
    "------<b>target_id:</b> of the target word.<br>\n",
    "------<b>context_ids:</b> of the context words. I have also used the zero padding to make total number of `window_size * 2` target words.<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed8ace6",
   "metadata": {},
   "source": [
    "# Important note \n",
    "context_ids te target id er id onusare value assing kora hoiche. 1 ta target er jnno jotota context ache toto value oi index a assing kora hocihe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d003cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_contexts(word_ids, window_size):\n",
    "    target_id = []\n",
    "    context_ids = []\n",
    "    \n",
    "    for ids in word_ids:\n",
    "        for index, word_id in enumerate(ids):\n",
    "            if not word_id:\n",
    "                continue\n",
    "                \n",
    "            window_start = max(0, index - window_size)\n",
    "            window_end = min(len(ids), index + window_size + 1)\n",
    "            \n",
    "            target_id.append(one_hot[word_id])\n",
    "            \n",
    "            zero_context_ids = np.zeros(vocabulary_size)\n",
    "            for window_index in range(window_start, window_end):\n",
    "                if window_index != index:\n",
    "                    zero_context_ids[ids[window_index]] += 1\n",
    "            context_ids.append(zero_context_ids)\n",
    "            del zero_context_ids\n",
    "            \n",
    "    return target_id, context_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01d545b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id, context_ids =  get_target_contexts(sentece_word_ids, window_size)\n",
    "data_info['Total target words'] = len(target_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad66e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(target_id, context_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff14e2",
   "metadata": {},
   "source": [
    "# CBOW Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71949fda",
   "metadata": {},
   "source": [
    "### Calculate the Sum of the Input Vectors \n",
    "\n",
    "I have pre calculated the sum for reducing the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a29a8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_cbow =  data_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a09cc4",
   "metadata": {},
   "source": [
    "### Split Training Testing Data\n",
    "\n",
    "`20%` data for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a9814",
   "metadata": {},
   "source": [
    "### Extending Keras Layer for Taking the Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d96af11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Average(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Average, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.divide(inputs, window_size*2)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a15ab51b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 400)]             0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding_cbow (Dense)   (None, 300)               120300    \n",
      "_________________________________________________________________\n",
      "average (Average)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 400)               120400    \n",
      "=================================================================\n",
      "Total params: 240,700\n",
      "Trainable params: 240,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 15:23:52.659402: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-11-24 15:23:52.662763: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(vocabulary_size, ))\n",
    "x = Dense(300, name='w2v_embedding_cbow')(inp)\n",
    "x = Average(200)(x)\n",
    "x = Dense(vocabulary_size, activation='softmax')(x)\n",
    "\n",
    "model_cbow = Model(inputs=inp, outputs=x)\n",
    "model_cbow.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6990626",
   "metadata": {},
   "source": [
    "### Fit the CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a971e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 15:23:54.924237: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-11-24 15:23:54.927515: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2021-11-24 15:23:55.076485: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 6325/11037 [================>.............] - ETA: 14s - loss: 36.1592 - accuracy: 0.1308"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6_/xv_wk6qn5k9g5yztczrn0zlh0000gn/T/ipykernel_991/2260245105.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtraining_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_cbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_loss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhistory_cbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1187\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \"\"\"\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    510\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \"\"\"\n\u001b[1;32m   1093\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflowEnv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def custom_loss_function(y_true, y_pred):\n",
    "    return tf.reduce_mean(-tf.math.reduce_sum(y_true * tf.math.log(y_pred), axis=[1]))\n",
    "\n",
    "\n",
    "training_started = datetime.datetime.now()\n",
    "model_cbow.compile(loss=custom_loss_function, optimizer=tf.optimizers.SGD(learning_rate=0.01), metrics=['accuracy'])\n",
    "history_cbow = model_cbow.fit(Y_train, X_train, epochs = 5, batch_size=50, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac711ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end = datetime.datetime.now()\n",
    "data_info_cbow['training time'] =  training_end - training_started\n",
    "data_info_cbow['cbow model training history'] = history_cbow.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5237b",
   "metadata": {},
   "source": [
    "### Save CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e8ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_model('cbow', model_cbow, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439e276",
   "metadata": {},
   "source": [
    "### Testing CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b76ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_cbow.evaluate(X_test, Y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_cbow['cbow model evaluatation loss'] = loss\n",
    "data_info_cbow['cbow model evaluatation accuracy'] = accuracy\n",
    "print('Evaluated loss',loss, 'and accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f931e73",
   "metadata": {},
   "source": [
    "### Save CBOW model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b359bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info('cbow',file_name, data_info_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cf891",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_cbow = model_cbow.get_layer('w2v_embedding_cbow').get_weights()[0]\n",
    "\n",
    "path_cbow = get_path_name('cbow') + 'vector_metadata/'\n",
    "\n",
    "\n",
    "if not os.path.exists(path_cbow):\n",
    "        os.makedirs(path_cbow)\n",
    "\n",
    "out_v = io.open(path_cbow+file_name+'vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open(path_cbow+file_name+'metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    vec = weights_cbow[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(id_to_word[index] + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542756c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('End of CBOW model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f497b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X_contexts                     \n",
    "# del X_test_cbow            \n",
    "# del X_train_cbow           \n",
    "# del Y_test_cbow            \n",
    "# del Y_train_cbow          \n",
    "# del accuracy               \n",
    "# del data_info_cbow        \n",
    "# del history_cbow                     \n",
    "# del inp                   \n",
    "# del loss                             \n",
    "# del pad_sequences          \n",
    "# del training_end           \n",
    "# del training_started      \n",
    "# del x   \n",
    "# del model_cbow \n",
    "# del out_m\n",
    "# del index                                  \n",
    "# del out_v                  \n",
    "# del path_cbow             \n",
    "# del vec                   \n",
    "# del weights_cbow           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db2008",
   "metadata": {},
   "source": [
    "## SKIP-GRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info_skip = data_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a63d3f",
   "metadata": {},
   "source": [
    "### Create SKIP-GRAMS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a285d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(vocabulary_size, ))\n",
    "x = Dense(500, name='w2v_embedding_skip', bias_initializer=tf.initializers.RandomNormal(stddev=1.0))(inp)\n",
    "x = Dense(vocabulary_size, bias_initializer=tf.initializers.RandomNormal(stddev=1.0), activation='softmax')(x)\n",
    "model_skip = Model(inputs=inp, outputs=x)\n",
    "model_skip.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464c747",
   "metadata": {},
   "source": [
    "### Fit skip-grams model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_started = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a04383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_function(y_true, y_pred):\n",
    "    return tf.reduce_mean(-tf.math.reduce_sum(y_true * tf.math.log(y_pred), axis=[1]))\n",
    "\n",
    "\n",
    "model_skip.compile(loss=custom_loss_function, optimizer=tf.optimizers.SGD(learning_rate=0.01), metrics=['accuracy'])\n",
    "history_skip = model_skip.fit(X_train, Y_train, epochs = 3, batch_size=50, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76fbef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end = datetime.datetime.now()\n",
    "data_info_skip['training time'] =  training_end - training_started\n",
    "data_info_skip['skip-grams model training history'] = history_skip.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddba413c",
   "metadata": {},
   "source": [
    "### Save SKIP_GRAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b192848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('skip', model_skip, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e86d1",
   "metadata": {},
   "source": [
    "### Test SKIP_GRAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc896b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_skip.evaluate(X_test, Y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99f4e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_info_skip['skip model evaluatation loss'] = loss\n",
    "data_info_skip['skip model evaluatation accuracy'] = accuracy\n",
    "print('Evaluated loss',loss, 'and accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc45cf",
   "metadata": {},
   "source": [
    "### Save Data info for Skip-Gram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6674ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info('skip',file_name, data_info_skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71849642",
   "metadata": {},
   "source": [
    "### Save Vector and Metadata For SKIP-GRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_skip = model_skip.get_layer('w2v_embedding_skip').get_weights()[0]\n",
    "\n",
    "path_skip = get_path_name('skip') + 'vector_metadata/'\n",
    "\n",
    "\n",
    "if not os.path.exists(path_skip):\n",
    "        os.makedirs(path_skip)\n",
    "\n",
    "out_v = io.open(path_skip+file_name+'vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open(path_skip+file_name+'metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index in range(1, vocabulary_size):\n",
    "    vec = weights_skip[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(id_to_word[index] + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f0ef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('End Skip-Gram Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad6c755",
   "metadata": {},
   "source": [
    "# Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773814e5",
   "metadata": {},
   "source": [
    "### CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450cbc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cbow = load_model('cbow', file_name)\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ab5cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_info('cbow', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae11c3",
   "metadata": {},
   "source": [
    "### Skip-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19878f7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "skip = load_model('skip', file_name)\n",
    "print(skip.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0facc9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_info('skip', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988dba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('THE END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62771ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflowEnv] *",
   "language": "python",
   "name": "conda-env-tensorflowEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
